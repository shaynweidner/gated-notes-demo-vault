<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="1" data-gn-md="%23%23%202.1%20Introduction"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="2" data-gn-md="The%20first%20three%20examples%20described%20in%20Chapter%201%20have%20several%20components%20in%20common.%20For%20each%2C%20there%20is%20a%20set%20of%20variables%20that%20might%20be%20denoted%20as%20inputs%2C%20which%20are%20measured%20or%20preset.%20These%20have%20some%20influence%20on%20one%20or%20more%20outputs.%20For%20each%20example%2C%20the%20goal%20is%20to%20use%20the%20inputs%20to%20predict%20the%20values%20of%20the%20outputs.%20This%20exercise%20is%20called%20supervised%20learning."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="3" data-gn-md="We%20have%20used%20the%20more%20modern%20language%20of%20machine%20learning.%20In%20the%20statistical%20literature%2C%20the%20inputs%20are%20often%20called%20the%20predictors%2C%20a%20term%20we%20will%20use%20interchangeably%20with%20inputs%2C%20and%20more%20classically%20the%20independent%20variables.%20In%20the%20pattern%20recognition%20literature%2C%20the%20term%20features%20is%20preferred%2C%20which%20we%20use%20as%20well.%20The%20outputs%20are%20called%20the%20responses%2C%20or%20classically%20the%20dependent%20variables."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="4" data-gn-md="%23%23%202.2%20Variable%20Types%20and%20Terminology"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="5" data-gn-md="The%20outputs%20vary%20in%20nature%20among%20the%20examples.%20In%20the%20glucose%20prediction%20example%2C%20the%20output%20is%20a%20quantitative%20measurement%2C%20where%20some%20measurements%20are%20bigger%20than%20others%2C%20and%20measurements%20close%20in%20value%20are%20close%20in%20nature.%20In%20the%20famous%20Iris%20discrimination%20example%20due%20to%20R.%20A.%20Fisher%2C%20the%20output%20is%20qualitative%20(species%20of%20Iris)%20and%20assumes%20values%20in%20a%20finite%20set%20%24G%20%3D%20%5C%7B%20%5Ctext%7BVirginica%7D%2C%20%5Ctext%7BSetosa%7D%2C%20%5Ctext%7BVersicolor%7D%20%5C%7D%24.%20In%20the%20handwritten%20digit%20example%2C%20the%20output%20is%20one%20of%2010%20different%20digit%20classes%3A%20%24G%20%3D%20%5C%7B%200%2C%201%2C%20%5Cldots%2C%209%20%5C%7D%24.%20%20In%20both%20of%20these%20there%20is%20no%20explicit%20ordering%20in%20the%20classes%2C%20and%20in%20fact%20often%20descriptive%20labels%20rather%20than%20numbers%20are%20used%20to%20denote%20the%20classes.%20Qualitative%20variables%20are%20also%20referred%20to%20as%20categorical%20or%20discrete%20variables%20as%20well%20as%20factors."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="6" data-gn-md="For%20both%20types%20of%20outputs%20it%20makes%20sense%20to%20think%20of%20using%20the%20inputs%20to%20predict%20the%20output.%20Given%20some%20specific%20atmospheric%20measurements%20today%20and%20yesterday%2C%20we%20want%20to%20predict%20the%20ozone%20level%20tomorrow.%20Given%20the%20grayscale%20values%20for%20the%20pixels%20of%20the%20digitized%20image%20of%20the%20handwritten%20digit%2C%20we%20want%20to%20predict%20its%20class%20label."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="7" data-gn-md="This%20distinction%20in%20output%20type%20has%20led%20to%20a%20naming%20convention%20for%20the%20prediction%20tasks%3A%20regression%20when%20we%20predict%20quantitative%20outputs%2C%20and%20classification%20when%20we%20predict%20qualitative%20outputs.%20We%20will%20see%20that%20these%20two%20tasks%20have%20a%20lot%20in%20common%2C%20and%20in%20particular%20both%20can%20be%20viewed%20as%20a%20task%20in%20function%20approximation."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="8" data-gn-md="Inputs%20also%20vary%20in%20measurement%20type%3B%20we%20can%20have%20some%20of%20each%20of%20qualitative%20and%20quantitative%20input%20variables.%20These%20have%20also%20led%20to%20distinctions%20in%20the%20types%20of%20methods%20that%20are%20used%20for%20prediction%3A%20some%20methods%20are%20defined%20most%20naturally%20for%20quantitative%20inputs%2C%20some%20most%20naturally%20for%20qualitative%20and%20some%20for%20both."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="9" data-gn-md="A%20third%20variable%20type%20is%20ordered%20categorical%2C%20such%20as%20small%2C%20medium%20and%20large%2C%20where%20there%20is%20an%20ordering%20between%20the%20values%2C%20but%20no%20metric%20notion%20is%20appropriate%20(the%20difference%20between%20medium%20and%20small%20need%20not%20be%20the%20same%20as%20that%20between%20large%20and%20medium).%20These%20are%20discussed%20further%20in%20Chapter%204."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="10" data-gn-md="Qualitative%20variables%20are%20typically%20represented%20numerically%20by%20codes.%20The%20easiest%20case%20is%20when%20there%20are%20only%20two%20classes%20or%20categories%2C%20such%20as%20%E2%80%9Csuccess%E2%80%9D%20or%20%E2%80%9Cfailure%2C%E2%80%9D%20%E2%80%9Csurvived%E2%80%9D%20or%20%E2%80%9Cdied.%E2%80%9D%20These%20are%20often%20represented%20by%20a%20single%20binary%20digit%20or%20bit%20as%200%20or%201%2C%20or%20else%20by%20%E2%88%921%20and%201.%20For%20reasons%20that%20will%20become%20apparent%2C%20such%20numeric%20codes%20are%20sometimes%20referred%20to%20as%20targets.%20%20When%20there%20are%20more%20than%20two%20categories%2C%20several%20alternatives%20are%20available.%20The%20most%20useful%20and%20commonly%20used%20coding%20is%20via%20dummy%20variables.%20Here%20a%20K-level%20qualitative%20variable%20is%20represented%20by%20a%20vector%20of%20K%20binary%20variables%20or%20bits%2C%20only%20one%20of%20which%20is%20%E2%80%9Con%E2%80%9D%20at%20a%20time.%20Although%20more%20compact%20coding%20schemes%20are%20possible%2C%20dummy%20variables%20are%20symmetric%20in%20the%20levels%20of%20the%20factor."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="11" data-gn-md="We%20will%20typically%20denote%20an%20input%20variable%20by%20the%20symbol%20%24X%24.%20If%20%24X%24%20is%20a%20vector%2C%20its%20components%20can%20be%20accessed%20by%20subscripts%20%24X_j%24.%20Quantitative%20outputs%20will%20be%20denoted%20by%20%24Y%24%2C%20and%20qualitative%20outputs%20by%20%24G%24%20(for%20group).%20We%20use%20uppercase%20letters%20such%20as%20%24X%24%2C%20%24Y%24%20or%20%24G%24%20when%20referring%20to%20the%20generic%20aspects%20of%20a%20variable.%20Observed%20values%20are%20written%20in%20lowercase%3B%20hence%20the%20%24i%24th%20observed%20value%20of%20%24X%24%20is%20written%20as%20%24x_i%24%20(where%20%24x_i%24%20is%20again%20a%20scalar%20or%20vector).%20Matrices%20are%20represented%20by%20bold%20uppercase%20letters%3B%20for%20example%2C%20a%20set%20of%20%24N%24%20input%20%24p%24-vectors%20%24x_i%24%2C%20%24i%20%3D%201%2C%20%5Cldots%2C%20N%24%20would%20be%20represented%20by%20the%20%24N%20%5Ctimes%20p%24%20matrix%20%24X%24.%20In%20general%2C%20vectors%20will%20not%20be%20bold%2C%20except%20when%20they%20have%20%24N%24%20components%3B%20this%20convention%20distinguishes%20a%20%24p%24-vector%20of%20inputs%20%24x_i%24%20for%20the%20%24i%24th%20observation%20from%20the%20%24N%24-vector%20%24x_j%24%20consists%20of%20all%20the%20observations%20on%20variable%20%24X_j%24.%20Since%20all%20vectors%20are%20assumed%20to%20be%20column%20vectors%2C%20the%20%24i%24th%20row%20of%20%24X%24%20is%20%24x_i%5ET%24%2C%20the%20vector%20transpose%20of%20%24x_i%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="12" data-gn-md="For%20the%20moment%20we%20can%20loosely%20state%20the%20learning%20task%20as%20follows%3A%20given%20the%20value%20of%20an%20input%20vector%20%24X%24%2C%20make%20a%20good%20prediction%20of%20the%20output%20%24Y%24%2C%20denoted%20by%20%24%5Chat%7BY%7D%24%20(pronounced%20%E2%80%9Cy-hat%E2%80%9D).%20If%20%24Y%24%20takes%20values%20in%20%24%5Cmathbb%7BR%7D%24%20then%20so%20should%20%24%5Chat%7BY%7D%24%3B%20likewise%20for%20categorical%20outputs%2C%20%24%5Chat%7BG%7D%24%20should%20take%20values%20in%20the%20same%20set%20%24G%24%20associated%20with%20%24G%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="13" data-gn-md="For%20a%20two-class%20%24G%24%2C%20one%20approach%20is%20to%20denote%20the%20binary%20coded%20target%20as%20%24Y%24%2C%20and%20then%20treat%20it%20as%20a%20quantitative%20output.%20The%20predictions%20%24%5Chat%7BY%7D%24%20will%20typically%20lie%20in%20%24%5B0%2C%201%5D%24%2C%20and%20we%20can%20assign%20to%20%24%5Chat%7BG%7D%24%20the%20class%20label%20according%20to%20whether%20%24%5Chat%7By%7D%20%3E%200.5%24.%20This%20approach%20generalizes%20to%20%24K%24-level%20qualitative%20outputs%20as%20well."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="14" data-gn-md="We%20need%20data%20to%20construct%20prediction%20rules%2C%20often%20a%20lot%20of%20it.%20We%20thus%20suppose%20we%20have%20available%20a%20set%20of%20measurements%20%24(x_i%2C%20y_i)%24%20or%20%24(x_i%2C%20g_i)%24%2C%20%24i%20%3D%201%2C%20%5Cldots%2C%20N%24%2C%20known%20as%20the%20training%20data%2C%20with%20which%20to%20construct%20our%20prediction%20rule."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="15" data-gn-md="%23%23%202.3%20Two%20Simple%20Approaches%20to%20Prediction%3A%20Least%20Squares%20and%20Nearest%20Neighbors"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="16" data-gn-md="In%20this%20section%20we%20develop%20two%20simple%20but%20powerful%20prediction%20methods%3A%20the%20linear%20model%20fit%20by%20least%20squares%20and%20the%20%24k%24-nearest-neighbor%20prediction%20rule.%20The%20linear%20model%20makes%20huge%20assumptions%20about%20structure%20and%20yields%20stable%20but%20possibly%20inaccurate%20predictions.%20The%20method%20of%20%24k%24-nearest%20neighbors%20makes%20very%20mild%20structural%20assumptions%3A%20its%20predictions%20are%20often%20accurate%20but%20can%20be%20unstable."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="17" data-gn-md="%23%23%23%202.3.1%20Linear%20Models%20and%20Least%20Squares"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="18" data-gn-md="The%20linear%20model%20has%20been%20a%20mainstay%20of%20statistics%20for%20the%20past%2030%20years%20and%20remains%20one%20of%20our%20most%20important%20tools.%20Given%20a%20vector%20of%20inputs%20%24X%5ET%20%3D%20(X_1%2C%20X_2%2C%20%5Cldots%2C%20X_p)%24%2C%20we%20predict%20the%20output%20%24Y%24%20via%20the%20model%0A%24%24%0A%5Chat%7BY%7D%20%3D%20%5Cbeta_0%20%2B%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7D%20X_j%20%5Chat%7B%5Cbeta%7D_j.%20%5Ctag%7B2.1%7D%0A%24%24%0AThe%20term%20%24%5Chat%7B%5Cbeta%7D_0%24%20is%20the%20intercept%2C%20also%20known%20as%20the%20bias%20in%20machine%20learning.%20Often%20it%20is%20convenient%20to%20include%20the%20constant%20variable%201%20in%20%24X%24%2C%20include%20%24%5Chat%7B%5Cbeta%7D_0%24%20in%20the%20vector%20of%20coefficients%20%24%5Chat%7B%5Cbeta%7D%24%2C%20and%20then%20write%20the%20linear%20model%20in%20vector%20form%20as%20an%20inner%20product%0A%24%24%0A%5Chat%7BY%7D%20%3D%20X%5ET%20%5Chat%7B%5Cbeta%7D.%20%5Ctag%7B2.2%7D%0A%24%24%0Awhere%20%24X%5ET%24%20denotes%20vector%20or%20matrix%20transpose%20(X%20being%20a%20column%20vector).%20Here%20we%20are%20modeling%20a%20single%20output%2C%20so%20%24Y%24%20is%20a%20scalar%3B%20in%20general%20%24Y%24%20can%20be%20a%20%24K%24-vector%2C%20in%20which%20case%20%24%5Cbeta%24%20would%20be%20a%20%24p%20%5Ctimes%20K%24%20matrix%20of%20coefficients.%20In%20the%20%24(p%20%2B%201)%24-dimensional%20input%E2%80%93output%20space%2C%20%24(X%2C%20Y)%24%20represents%20a%20hyperplane.%20If%20the%20constant%20is%20included%20in%20%24X%24%2C%20then%20the%20hyperplane%20includes%20the%20origin%20and%20is%20a%20subspace%3B%20if%20not%2C%20it%20is%20an%20affine%20set%20cutting%20the%20%24Y%24-axis%20at%20the%20point%20%24(0%2C%20%5Chat%7B%5Cbeta%7D_0)%24.%20From%20now%20on%20we%20assume%20that%20the%20intercept%20is%20included%20in%20%24%5Chat%7B%5Cbeta%7D%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="19" data-gn-md="Viewed%20as%20a%20function%20over%20the%20%24p%24-dimensional%20input%20space%2C%20%24f(X)%20%3D%20X%20%5Chat%7B%5Cbeta%7D'%24%20is%20linear%2C%20and%20the%20gradient%20%24f'(X)%20%3D%20%5Chat%7B%5Cbeta%7D%24%20is%20a%20vector%20in%20input%20space%20that%20points%20in%20the%20steepest%20uphill%20direction."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="20" data-gn-md="How%20do%20we%20fit%20the%20linear%20model%20to%20a%20set%20of%20training%20data%3F%20There%20are%20many%20different%20methods%2C%20but%20by%20far%20the%20most%20popular%20is%20the%20method%20of%20least%20squares.%20In%20this%20approach%2C%20we%20pick%20the%20coefficients%20%24%5Cbeta%24%20to%20minimize%20the%20residual%20sum%20of%20squares%0A%24%24%0ARSS(%5Cbeta)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%5E2.%20%5Ctag%7B2.3%7D%0A%24%24%0A%24RSS(%5Cbeta)%24%20is%20a%20quadratic%20function%20of%20the%20parameters%2C%20and%20hence%20its%20minimum%20always%20exists%2C%20but%20may%20not%20be%20unique.%20The%20solution%20is%20easiest%20to%20characterize%20in%20matrix%20notation.%20We%20can%20write%0A%24%24%0ARSS(%5Cbeta)%20%3D%20(y%20-%20X%20%5Cbeta)%5ET%20(y%20-%20X%20%5Cbeta)%2C%20%5Ctag%7B2.4%7D%0A%24%24%0Awhere%20%24X%24%20is%20an%20%24N%20%5Ctimes%20p%24%20matrix%20with%20each%20row%20an%20input%20vector%2C%20and%20%24y%24%20is%20an%20%24N%24-vector%20of%20the%20outputs%20in%20the%20training%20set.%20Differentiating%20w.r.t.%20%24%5Cbeta%24%20we%20get%20the%20normal%20equations%0A%24%24%0AX%5ET%20(y%20-%20X%20%5Cbeta)%20%3D%200.%20%5Ctag%7B2.5%7D%0A%24%24%0AIf%20%24X%5ET%20X%24%20is%20nonsingular%2C%20then%20the%20unique%20solution%20is%20given%20by%0A%24%24%0A%5Chat%7B%5Cbeta%7D%20%3D%20(X%5ET%20X)%5E%7B-1%7D%20X%5ET%20y%2C%20%5Ctag%7B2.6%7D%0A%24%24%0Aand%20the%20fitted%20value%20at%20the%20%24i%24th%20input%20%24x_i%24%20is%20%24%5Chat%7By%7D_i%20%3D%20%5Chat%7By%7D(x_i)%20%3D%20x_i%20%5Chat%7B%5Cbeta%7D%24.%20At%20an%20arbitrary%20input%20%24x_0%24%2C%20the%20prediction%20is%20%24%5Chat%7By%7D(x_0)%20%3D%20x_0%20%5Chat%7B%5Cbeta%7D%24.%20The%20entire%20fitted%20surface%20is%20characterized%20by%20the%20%24p%24%20parameters%20%24%5Cbeta%24.%20Intuitively%2C%20it%20seems%20that%20we%20do%20not%20need%20a%20very%20large%20data%20set%20to%20fit%20such%20a%20model."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="21" data-gn-md="Let%E2%80%99s%20look%20at%20an%20example%20of%20the%20linear%20model%20in%20a%20classification%20context."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="22" data-gn-md="!%5B%5BPasted%20image%2020250824194650.png%5D%5D%0A**FIGURE%202.1.**%20A%20classification%20example%20in%20two%20dimensions.%20The%20classes%20are%20coded%20as%20a%20binary%20variable%20(BLUE%20%3D%200%2C%20ORANGE%20%3D%201)%2C%20and%20then%20fit%20by%20linear%20regression.%20The%20line%20is%20the%20decision%20boundary%20defined%20by%20%24x%5ET%20%5Chat%7B%5Cbeta%7D%20%3D%200.5%24.%20The%20orange%20shaded%20region%20denotes%20that%20part%20of%20input%20space%20classified%20as%20ORANGE%2C%20while%20the%20blue%20region%20is%20classified%20as%20BLUE."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="23" data-gn-md="Figure%202.1%20shows%20a%20scatterplot%20of%20training%20data%20on%20a%20pair%20of%20inputs%20%24X_1%24%20and%20%24X_2%24.%20The%20data%20are%20simulated%2C%20and%20for%20the%20present%20the%20simulation%20model%20is%20not%20important.%20The%20output%20class%20variable%20%24G%24%20has%20the%20values%20BLUE%20or%20ORANGE%2C%20and%20is%20represented%20as%20such%20in%20the%20scatterplot.%20There%20are%20100%20points%20in%20each%20of%20the%20two%20classes.%20The%20linear%20regression%20model%20was%20fit%20to%20these%20data%2C%20with%20the%20response%20%24Y%24%20coded%20as%200%20for%20BLUE%20and%201%20for%20ORANGE.%20The%20fitted%20values%20%24%5Chat%7BY%7D%24%20are%20converted%20to%20a%20fitted%20class%20variable%20%24%5Chat%7BG%7D%24%20according%20to%20the%20rule%0A%24%24%0A%5Chat%7BG%7D%20%3D%20%0A%5Cbegin%7Bcases%7D%20%0AORANGE%20%26%20%5Ctext%7Bif%20%7D%20%5Chat%7BY%7D%20%3E%200.5%2C%20%5C%5C%20%0ABLUE%20%26%20%5Ctext%7Bif%20%7D%20%5Chat%7BY%7D%20%5Cleq%200.5.%20%0A%5Cend%7Bcases%7D%20%5Ctag%7B2.7%7D%0A%24%24%0AThe%20set%20of%20points%20in%20%24%5Cmathbb%7BR%7D%5E2%24%20classified%20as%20ORANGE%20corresponds%20to%20%24%5C%7Bx%20%3A%20x%5ET%20%5Chat%7B%5Cbeta%7D%20%3E%200.5%5C%7D%24%2C%20indicated%20in%20Figure%202.1%2C%20and%20the%20two%20predicted%20classes%20are%20separated%20by%20the%20decision%20boundary%20%24%5C%7Bx%20%3A%20x%5ET%20%5Chat%7B%5Cbeta%7D%20%3D%200.5%5C%7D%24%2C%20which%20is%20linear%20in%20this%20case.%20We%20see%20that%20for%20these%20data%20there%20are%20several%20misclassifications%20on%20both%20sides%20of%20the%20decision%20boundary.%20Perhaps%20our%20linear%20model%20is%20too%20rigid%E2%80%94or%20are%20such%20errors%20unavoidable%3F%20Remember%20that%20these%20are%20errors%20on%20the%20training%20data%20itself%2C%20and%20we%20have%20not%20said%20where%20the%20constructed%20data%20came%20from.%20Consider%20the%20two%20possible%20scenarios%3A%0A1.%20**Scenario%201%3A**%20The%20training%20data%20in%20each%20class%20were%20generated%20from%20bivariate%20Gaussian%20distributions%20with%20uncorrelated%20components%20and%20different%20means.%0A2.%20**Scenario%202%3A**%20The%20training%20data%20in%20each%20class%20came%20from%20a%20mixture%20of%2010%20low-variance%20Gaussian%20distributions%2C%20with%20individual%20means%20themselves%20distributed%20as%20Gaussian."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="24" data-gn-md="A%20mixture%20of%20Gaussians%20is%20best%20described%20in%20terms%20of%20the%20generative%20model.%20One%20first%20generates%20a%20discrete%20variable%20that%20determines%20which%20of%20the%20component%20Gaussians%20to%20use%2C%20and%20then%20generates%20an%20observation%20from%20the%20chosen%20density.%20In%20the%20case%20of%20one%20Gaussian%20per%20class%2C%20we%20will%20see%20in%20Chapter%204%20that%20a%20linear%20decision%20boundary%20is%20the%20best%20one%20can%20do%2C%20and%20that%20our%20estimate%20is%20almost%20optimal.%20The%20region%20of%20overlap%20is%20inevitable%2C%20and%20future%20data%20to%20be%20predicted%20will%20be%20plagued%20by%20this%20overlap%20as%20well."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="25" data-gn-md="In%20the%20case%20of%20mixtures%20of%20tightly%20clustered%20Gaussians%2C%20the%20story%20is%20different.%20A%20linear%20decision%20boundary%20is%20unlikely%20to%20be%20optimal%2C%20and%20in%20fact%20is%20not.%20The%20optimal%20decision%20boundary%20is%20nonlinear%20and%20disjoint%2C%20and%20as%20such%20will%20be%20much%20more%20difficult%20to%20obtain."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="26" data-gn-md="We%20now%20look%20at%20another%20classification%20and%20regression%20procedure%20that%20is%20in%20some%20sense%20at%20the%20opposite%20end%20of%20the%20spectrum%20to%20the%20linear%20model%2C%20and%20far%20better%20suited%20to%20the%20second%20scenario."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="27" data-gn-md="%23%23%202.3.2%20Nearest-Neighbor%20Methods"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="28" data-gn-md="Nearest-neighbor%20methods%20use%20those%20observations%20in%20the%20training%20set%20%24T%24%20closest%20in%20input%20space%20to%20%24x%24%20to%20form%20%24%5Chat%7BY%7D%24.%20Specifically%2C%20the%20%24k%24-nearest%20neighbor%20fit%20for%20%24%5Chat%7BY%7D%24%20is%20defined%20as%20follows%3A%0A%24%24%0A%5Chat%7BY%7D(x)%20%3D%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_%7Bx_i%20%5Cin%20N_k(x)%7D%20y_i%2C%20%5Ctag%7B2.8%7D%0A%24%24%0Awhere%20%24N_k(x)%24%20is%20the%20neighborhood%20of%20%24x%24%20defined%20by%20the%20%24k%24%20closest%20points%20%24x_i%24%20in%20the%20training%20sample.%20Closeness%20implies%20a%20metric%2C%20which%20for%20the%20moment%20we%20assume%20is%20Euclidean%20distance.%20So%2C%20in%20words%2C%20we%20find%20the%20%24k%24%20observations%20with%20%24x_i%24%20closest%20to%20%24x%24%20in%20input%20space%2C%20and%20average%20their%20responses."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="29" data-gn-md="In%20Figure%202.2%2C%20we%20use%20the%20same%20training%20data%20as%20in%20Figure%202.1%2C%20and%20use%2015-nearest-neighbor%20averaging%20of%20the%20binary%20coded%20response%20as%20the%20method%20of%20fitting.%20Thus%20%24%5Chat%7BY%7D%24%20is%20the%20proportion%20of%20ORANGE%E2%80%99s%20in%20the%20neighborhood%2C%20and%20so%20assigning%20class%20ORANGE%20to%20%24G%24%20if%20%24%5Chat%7BY%7D%20%3E%200.5%24%20amounts%20to%20a%20majority%20vote%20in%20the%20neighborhood.%20The%20colored%20regions%20indicate%20all%20those%20points%20in%20input%20space%20classified%20as%20BLUE%20or%20ORANGE%20by%20such%20a%20rule%2C%20in%20this%20case%20found%20by%20evaluating%20the%20procedure%20on%20a%20fine%20grid%20in%20input%20space.%20We%20see%20that%20the%20decision%20boundaries%20that%20separate%20the%20BLUE%20from%20the%20ORANGE%20regions%20are%20far%20more%20irregular%2C%20and%20respond%20to%20local%20clusters%20where%20one%20class%20dominates."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="30" data-gn-md="Figure%202.3%20shows%20the%20results%20for%201-nearest-neighbor%20classification%3A%20%24%5Chat%7BY%7D%24%20is%20assigned%20the%20value%20%24y_%5Cell%24%20of%20the%20closest%20point%20%24x_%5Cell%24%20to%20%24x%24%20in%20the%20training%20data.%20In%20this%20case%2C%20the%20regions%20of%20classification%20can%20be%20computed%20relatively%20easily%2C%20and%20correspond%20to%20a%20Voronoi%20tessellation%20of%20the%20training%20data.%20Each%20point%20%24x_i%24%20has%20an%20associated%20tile%20bounding%20the%20region%20for%20which%20it%20is%20the%20closest%20input%20point.%20For%20all%20points%20%24x%24%20in%20the%20tile%2C%20%24G(x)%20%3D%20g_i%24.%20The%20decision%20boundary%20is%20even%20more%20irregular%20than%20before."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="31" data-gn-md="The%20method%20of%20%24k%24-nearest-neighbor%20averaging%20is%20defined%20in%20exactly%20the%20same%20way%20for%20regression%20of%20a%20quantitative%20output%20%24%5Chat%7BY%7D%24%2C%20although%20%24k%20%3D%201%24%20would%20be%20an%20unlikely%20choice."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="32" data-gn-md="!%5B%5BPasted%20image%2020250825101900.png%5D%5D%0A**Figure%202.2.**%20The%20same%20classification%20example%20in%20two%20dimensions%20as%20in%20Figure%202.1.%20The%20classes%20are%20coded%20as%20a%20binary%20variable%20(BLUE%20%3D%200%2C%20ORANGE%20%3D%201)%20and%20then%20fit%20by%2015-nearest-neighbor%20averaging%20as%20in%20(2.8).%20The%20predicted%20class%20is%20hence%20chosen%20by%20majority%20vote%20amongst%20the%2015-nearest%20neighbors."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="33" data-gn-md="In%20Figure%202.2%20we%20see%20that%20far%20fewer%20training%20observations%20are%20misclassified%20than%20in%20Figure%202.1.%20This%20should%20not%20give%20us%20too%20much%20comfort%2C%20though%2C%20since%20in%20Figure%202.3%20none%20of%20the%20training%20data%20are%20misclassified.%20A%20little%20thought%20suggests%20that%20for%20%24k%24-nearest-neighbor%20fits%2C%20the%20error%20on%20the%20training%20data%20should%20be%20approximately%20an%20increasing%20function%20of%20%24k%24%2C%20and%20will%20always%20be%200%20for%20%24k%20%3D%201%24.%20An%20independent%20test%20set%20would%20give%20us%20a%20more%20satisfactory%20means%20for%20comparing%20the%20different%20methods."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="34" data-gn-md="It%20appears%20that%20%24k%24-nearest-neighbor%20fits%20have%20a%20single%20parameter%2C%20the%20number%20of%20neighbors%20%24k%24%2C%20compared%20to%20the%20%24p%24%20parameters%20in%20least-squares%20fits.%20Although%20this%20is%20the%20case%2C%20we%20will%20see%20that%20the%20effective%20number%20of%20parameters%20of%20%24k%24-nearest%20neighbors%20is%20%24%5Cfrac%7BN%7D%7Bk%7D%24%20and%20is%20generally%20bigger%20than%20%24p%24%2C%20and%20decreases%20with%20increasing%20%24k%24.%20To%20get%20an%20idea%20of%20why%2C%20note%20that%20if%20the%20neighborhoods%20were%20nonoverlapping%2C%20there%20would%20be%20%24%5Cfrac%7BN%7D%7Bk%7D%24%20neighborhoods%20and%20we%20would%20fit%20one%20parameter%20(a%20mean)%20in%20each%20neighborhood."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="35" data-gn-md="It%20is%20also%20clear%20that%20we%20cannot%20use%20sum-of-squared%20errors%20on%20the%20training%20set%20as%20a%20criterion%20for%20picking%20%24k%24%2C%20since%20we%20would%20always%20pick%20%24k%20%3D%201%24!%20It%20would%20seem%20that%20%24k%24-nearest-neighbor%20methods%20would%20be%20more%20appropriate%20for%20the%20mixture%20Scenario%202%20described%20above%2C%20while%20for%20Gaussian%20data%20the%20decision%20boundaries%20of%20%24k%24-nearest%20neighbors%20would%20be%20unnecessarily%20noisy."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="36" data-gn-md="!%5B%5BPasted%20image%2020250825101934.png%5D%5D%0A**Figure%202.3.**%20The%20same%20classification%20example%20in%20two%20dimensions%20as%20in%20Figure%202.1.%20The%20classes%20are%20coded%20as%20a%20binary%20variable%20(BLUE%20%3D%200%2C%20ORANGE%20%3D%201)%2C%20and%20then%20predicted%20by%201-nearest-neighbor%20classification."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="37" data-gn-md="%23%23%202.3.3%20From%20Least%20Squares%20to%20Nearest%20Neighbors"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="38" data-gn-md="The%20linear%20decision%20boundary%20from%20least%20squares%20is%20very%20smooth%20and%20apparently%20stable%20to%20fit.%20It%20does%20appear%20to%20rely%20heavily%20on%20the%20assumption%20that%20a%20linear%20decision%20boundary%20is%20appropriate.%20In%20language%20we%20will%20develop%20shortly%2C%20it%20has%20low%20variance%20and%20potentially%20high%20bias."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="39" data-gn-md="On%20the%20other%20hand%2C%20the%20%24k%24-nearest-neighbor%20procedures%20do%20not%20appear%20to%20rely%20on%20any%20stringent%20assumptions%20about%20the%20underlying%20data%20and%20can%20adapt%20to%20any%20situation.%20However%2C%20any%20particular%20subregion%20of%20the%20decision%20boundary%20depends%20on%20a%20handful%20of%20input%20points%20and%20their%20particular%20positions%2C%20and%20is%20thus%20wiggly%20and%20unstable%E2%80%94high%20variance%20and%20low%20bias."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="40" data-gn-md="Each%20method%20has%20its%20own%20situations%20for%20which%20it%20works%20best%3B%20in%20particular%2C%20linear%20regression%20is%20more%20appropriate%20for%20Scenario%201%20above%2C%20while%20nearest%20neighbors%20are%20more%20suitable%20for%20Scenario%202.%20The%20time%20has%20come%20to%20expose%20the%20oracle!%20The%20data%20in%20fact%20were%20simulated%20from%20a%20model%20somewhere%20between%20the%20two%2C%20but%20closer%20to%20Scenario%202.%20First%2C%20we%20generated%2010%20means%20%24m_k%24%20from%20a%20bivariate%20Gaussian%20distribution%20%24N((1%2C%200)%2C%20I)%24%20and%20labeled%20this%20class%20BLUE.%20Similarly%2C%2010%20more%20were%20drawn%20from%20%24N((0%2C%201)%2C%20I)%24%20and%20labeled%20class%20ORANGE.%20Then%20for%20each%20class%2C%20we%20generated%20100%20observations%20as%20follows%3A%20for%20each%20observation%2C%20we%20picked%20an%20%24m_k%24%20at%20random%20with%20probability%20%24%5Cfrac%7B1%7D%7B10%7D%24%2C%20and%20then%20generated%20a%20%24N(m_k%2C%20I%2F5)%24%2C%20thus%20leading%20to%20a%20mixture%20of%20Gaussian%20clusters%20for%20each%20class.%20Figure%202.4%20shows%20the%20results%20of%20classifying%2010%2C000%20new%20observations%20generated%20from%20the%20model.%20We%20compare%20the%20results%20for%20least%20squares%20and%20those%20for%20%24k%24-nearest%20neighbors%20for%20a%20range%20of%20values%20of%20%24k%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="41" data-gn-md="!%5B%5BPasted%20image%2020250825102023.png%5D%5D%0A**FIGURE%202.4.**%20Misclassification%20curves%20for%20the%20simulation%20example%20used%20in%20Figures%202.1%2C%202.2%2C%20and%202.3.%20A%20single%20training%20sample%20of%20size%20200%20was%20used%2C%20and%20a%20test%20sample%20of%20size%2010%2C000.%20The%20orange%20curves%20are%20test%20and%20the%20blue%20are%20training%20error%20for%20%24k%24-nearest-neighbor%20classification.%20The%20results%20for%20linear%20regression%20are%20the%20bigger%20orange%20and%20blue%20squares%20at%20three%20degrees%20of%20freedom.%20The%20purple%20line%20is%20the%20optimal%20Bayes%20error%20rate."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="42" data-gn-md="A%20large%20subset%20of%20the%20most%20popular%20techniques%20in%20use%20today%20are%20variants%20of%20these%20two%20simple%20procedures.%20In%20fact%2C%201-nearest-neighbor%2C%20the%20simplest%20of%20all%2C%20captures%20a%20large%20percentage%20of%20the%20market%20for%20low-dimensional%20problems.%20%20The%20following%20list%20describes%20some%20ways%20in%20which%20these%20simple%20procedures%20have%20been%20enhanced%3A%0A-%20Kernel%20methods%20use%20weights%20that%20decrease%20smoothly%20to%20zero%20with%20distance%20from%20the%20target%20point%2C%20rather%20than%20the%20effective%200%2F1%20weights%20used%20by%20%24k%24-nearest%20neighbors.%0A-%20In%20high-dimensional%20spaces%2C%20the%20distance%20kernels%20are%20modified%20to%20emphasize%20some%20variable%20more%20than%20others.%0A-%20Local%20regression%20fits%20linear%20models%20by%20locally%20weighted%20least%20squares%2C%20rather%20than%20fitting%20constants%20locally.%0A-%20Linear%20models%20fit%20to%20a%20basis%20expansion%20of%20the%20original%20inputs%20allow%20arbitrarily%20complex%20models.%0A-%20Projection%20pursuit%20and%20neural%20network%20models%20consist%20of%20sums%20of%20non-linearly%20transformed%20linear%20models."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="43" data-gn-md="%23%23%202.4%20Statistical%20Decision%20Theory"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="44" data-gn-md="In%20this%20section%2C%20we%20develop%20a%20small%20amount%20of%20theory%20that%20provides%20a%20framework%20for%20developing%20models%20such%20as%20those%20discussed%20informally%20so%20far.%20We%20first%20consider%20the%20case%20of%20a%20quantitative%20output%20and%20place%20ourselves%20in%20the%20world%20of%20random%20variables%20and%20probability%20spaces.%20Let%20%24X%20%5Cin%20%5Cmathbb%7BR%7D%5Ep%24%20denote%20a%20real%20valued%20random%20input%20vector%2C%20and%20%24Y%20%5Cin%20%5Cmathbb%7BR%7D%24%20a%20real%20valued%20random%20output%20variable%2C%20with%20joint%20distribution%20%24%5Ctext%7BPr%7D(X%2C%20Y)%24.%20We%20seek%20a%20function%20%24f(X)%24%20for%20predicting%20%24Y%24%20given%20values%20of%20the%20input%20%24X%24.%20This%20theory%20requires%20a%20loss%20function%20%24L(Y%2C%20f(X))%24%20for%20penalizing%20errors%20in%20prediction%2C%20and%20by%20far%20the%20most%20common%20and%20convenient%20is%20squared%20error%20loss%3A%20%24L(Y%2C%20f(X))%20%3D%20(Y%20-%20f(X))%5E2%24.%20This%20leads%20us%20to%20a%20criterion%20for%20choosing%20%24f%24%3A%0A%24%24EPE(f)%20%3D%20E(Y%20-%20f(X))%5E2%20%5Ctag%7B2.9%7D%24%24%0A%24%24%3D%20%5Cint%20%5By%20-%20f(x)%5D%5E2%20%5Ctext%7BPr%7D(dx%2C%20dy)%2C%20%5Ctag%7B2.10%7D%24%24%0Athe%20expected%20(squared)%20prediction%20error.%20By%20conditioning%20on%20%24X%24%2C%20we%20can%20write%20EPE%20as%0A%24%24EPE(f)%20%3D%20E_X%20E_%7BY%7CX%7D%5B(Y%20-%20f(X))%5E2%20%7C%20X%5D%20%5Ctag%7B2.11%7D%24%24%0Aand%20we%20see%20that%20it%20suffices%20to%20minimize%20EPE%20pointwise%3A%0A%24%24f(x)%20%3D%20%5Carg%5Cmin_%7Bc%7D%20E_%7BY%7CX%7D%5B(Y%20-%20c)%5E2%20%7C%20X%20%3D%20x%5D.%20%5Ctag%7B2.12%7D%24%24%0AThe%20solution%20is%20%0A%24%24f(x)%20%3D%20E(Y%20%7C%20X%20%3D%20x)%2C%20%5Ctag%7B2.13%7D%24%24%0Athe%20conditional%20expectation%2C%20also%20known%20as%20the%20regression%20function.%20Thus%2C%20the%20best%20prediction%20of%20%24Y%24%20at%20any%20point%20%24X%20%3D%20x%24%20is%20the%20conditional%20mean%2C%20when%20best%20is%20measured%20by%20average%20squared%20error."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="45" data-gn-md="The%20nearest-neighbor%20methods%20attempt%20to%20directly%20implement%20this%20recipe%20using%20the%20training%20data.%20At%20each%20point%20%24x%24%2C%20we%20might%20ask%20for%20the%20average%20of%20all%20those%20%24y_i%24's%20with%20input%20%24x_i%20%3D%20x%24.%20Since%20there%20is%20typically%20at%20most%20one%20observation%20at%20any%20point%20%2C%20we%20settle%20for%20%0A%24%24%5Chat%7Bf%7D(x)%20%3D%20%5Ctext%7BAve%7D(y_i%20%7C%20x_i%20%5Cin%20N_k(x))%2C%20%5Cquad%20(2.14)%24%24%0Awhere%20%E2%80%9CAve%E2%80%9D%20denotes%20average%2C%20and%20%24N_k(x)%24%20is%20the%20neighborhood%20containing%20the%20%24k%24%20points%20in%20%24T%24%20closest%20to%20%24x%24.%20Two%20approximations%20are%20happening%20here%3A%0A-%20Expectation%20is%20approximated%20by%20averaging%20over%20sample%20data%3B%0A-%20Conditioning%20at%20a%20point%20is%20relaxed%20to%20conditioning%20on%20some%20region%20%E2%80%9Cclose%E2%80%9D%20to%20the%20target%20point.%0AFor%20large%20training%20sample%20size%20%24N%24%2C%20the%20points%20in%20the%20neighborhood%20are%20likely%20to%20be%20close%20to%20%24x%24%2C%20and%20as%20%24k%24%20gets%20large%20the%20average%20will%20get%20more%20stable.%20In%20fact%2C%20under%20mild%20regularity%20conditions%20on%20the%20joint%20probability%20distribution%20%24%5Ctext%7BPr%7D(X%2C%20Y)%24%2C%20one%20can%20show%20that%20as%20%24N%2C%20k%20%5Cto%20%5Cinfty%24%20such%20that%20%24k%2FN%20%5Cto%200%24%2C%20%24%5Chat%7Bf%7D(x)%20%5Cto%20E(Y%20%7C%20X%20%3D%20x)%24.%20In%20light%20of%20this%2C%20why%20look%20further%2C%20since%20it%20seems%20we%20have%20a%20universal%20approximator%3F%20We%20often%20do%20not%20have%20very%20large%20samples.%20If%20the%20linear%20or%20some%20more%20structured%20model%20is%20appropriate%2C%20then%20we%20can%20usually%20get%20a%20more%20stable%20estimate%20than%20%24k%24-nearest%20neighbors%2C%20although%20such%20knowledge%20has%20to%20be%20learned%20from%20the%20data%20as%20well.%20There%20are%20other%20problems%20though%2C%20sometimes%20disastrous.%20In%20Section%202.5%20we%20see%20that%20as%20the%20dimension%20%24p%24%20gets%20large%2C%20so%20does%20the%20metric%20size%20of%20the%20%24k%24-nearest%20neighborhood.%20So%20settling%20for%20nearest%20neighborhood%20as%20a%20surrogate%20for%20conditioning%20will%20fail%20us%20miserably.%20The%20convergence%20above%20still%20holds%2C%20but%20the%20rate%20of%20convergence%20decreases%20as%20the%20dimension%20increases."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="46" data-gn-md="How%20does%20linear%20regression%20fit%20into%20this%20framework%3F%20The%20simplest%20explanation%20is%20that%20one%20assumes%20that%20the%20regression%20function%20%24f(x)%24%20is%20approximately%20linear%20in%20its%20arguments%3A%0A%24%24f(x)%20%5Capprox%20x%5ET%20%5Cbeta.%20%5Cquad%20(2.15)%24%24%0AThis%20is%20a%20model-based%20approach%E2%80%94we%20specify%20a%20model%20for%20the%20regression%20function.%20Plugging%20this%20linear%20model%20for%20%24f(x)%24%20into%20EPE%20(2.9)%20and%20differentiating%2C%20we%20can%20solve%20for%20%24%5Cbeta%24%20theoretically%3A%0A%24%24%5Cbeta%20%3D%20%5BE(XX%5ET)%5D%5E%7B-1%7DE(XY).%20%5Cquad%20(2.16)%24%24%0ANote%20we%20have%20not%20conditioned%20on%20%24X%24%3B%20rather%20we%20have%20used%20our%20knowledge%20of%20the%20functional%20relationship%20to%20pool%20over%20values%20of%20%24X%24.%20The%20least%20squares%20solution%20(2.6)%20amounts%20to%20replacing%20the%20expectation%20in%20(2.16)%20by%20averages%20over%20the%20training%20data."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="47" data-gn-md="So%20both%20%24k%24-nearest%20neighbors%20and%20least%20squares%20end%20up%20approximating%20conditional%20expectations%20by%20averages.%20But%20they%20differ%20dramatically%20in%20terms%20of%20model%20assumptions%3A%0A-%20Least%20squares%20assumes%20%24f(x)%24%20is%20well%20approximated%20by%20a%20globally%20linear%20function.%0A-%20k-nearest%20neighbors%20assumes%20%24f(x)%24%20is%20well%20approximated%20by%20a%20locally%20constant%20function."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="48" data-gn-md="Although%20the%20latter%20seems%20more%20palatable%2C%20we%20have%20already%20seen%20that%20we%20may%20pay%20a%20price%20for%20this%20flexibility.%20Many%20of%20the%20more%20modern%20techniques%20described%20in%20this%20book%20are%20model%20based%2C%20although%20far%20more%20flexible%20than%20the%20rigid%20linear%20model.%20For%20example%2C%20additive%20models%20assume%20that%20%0A%24%24%0Af(X)%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7D%20f_j(X_j).%20%5Ctag%7B2.17%7D%0A%24%24%0AThis%20retains%20the%20additivity%20of%20the%20linear%20model%2C%20but%20each%20coordinate%20function%20%24f_j%24%20is%20arbitrary.%20It%20turns%20out%20that%20the%20optimal%20estimate%20for%20the%20additive%20model%20uses%20techniques%20such%20as%20k-nearest%20neighbors%20to%20approximate%20univariate%20conditional%20expectations%20simultaneously%20for%20each%20of%20the%20coordinate%20functions.%20Thus%20the%20problems%20of%20estimating%20a%20conditional%20expectation%20in%20high%20dimensions%20are%20swept%20away%20in%20this%20case%20by%20imposing%20some%20(often%20unrealistic)%20model%20assumptions%2C%20in%20this%20case%20additivity."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="49" data-gn-md="Are%20we%20happy%20with%20the%20criterion%20(2.11)%3F%20What%20happens%20if%20we%20replace%20the%20%24L_2%24%20loss%20function%20with%20the%20%24L_1%24%3A%20%24E%20%7C%20Y%20-%20f(X)%20%7C%20%24%3F%20The%20solution%20in%20this%20case%20is%20the%20conditional%20median%2C%0A%24%24%0A%5Chat%7Bf%7D(x)%20%3D%20%5Ctext%7Bmedian%7D(Y%20%7C%20X%20%3D%20x)%2C%20%5Ctag%7B2.18%7D%0A%24%24%0Awhich%20is%20a%20different%20measure%20of%20location%2C%20and%20its%20estimates%20are%20more%20robust%20than%20those%20for%20the%20conditional%20mean.%20%24L_1%24%20criteria%20have%20discontinuities%20in%20their%20derivatives%2C%20which%20have%20hindered%20their%20widespread%20use.%20Other%20more%20resistant%20loss%20functions%20will%20be%20mentioned%20in%20later%20chapters%2C%20but%20squared%20error%20is%20analytically%20convenient%20and%20the%20most%20popular."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="50" data-gn-md="What%20do%20we%20do%20when%20the%20output%20is%20a%20categorical%20variable%20%24G%24%3F%20The%20same%20paradigm%20works%20here%2C%20except%20we%20need%20a%20different%20loss%20function%20for%20penalizing%20prediction%20errors.%20An%20estimate%20%24%5Chat%7BG%7D%24%20will%20assume%20values%20in%20%24G%24%2C%20the%20set%20of%20possible%20classes.%20Our%20loss%20function%20can%20be%20represented%20by%20a%20%24K%20%5Ctimes%20K%24%20matrix%20%24L%24%2C%20where%20%24K%20%3D%20%5Ctext%7Bcard%7D(G)%24.%20%24L%24%20will%20be%20zero%20on%20the%20diagonal%20and%20nonnegative%20elsewhere%2C%20where%20%24L(k%2C%20%5Cell)%24%20is%20the%20price%20paid%20for%20classifying%20an%20observation%20belonging%20to%20class%20%24G_k%24%20as%20%24G_%5Cell%24.%20Most%20often%20we%20use%20the%20zero%E2%80%93one%20loss%20function%2C%20where%20all%20misclassifications%20are%20charged%20a%20single%20unit.%20The%20expected%20prediction%20error%20is%20%0A%24%24%0A%5Chat%7BEPE%7D%20%3D%20E%5BL(G%2C%20%5Chat%7BG%7D(X))%5D%2C%20%5Ctag%7B2.19%7D%0A%24%24%0Awhere%20again%20the%20expectation%20is%20taken%20with%20respect%20to%20the%20joint%20distribution%20%24Pr(G%2C%20X)%24.%20Again%20we%20condition%2C%20and%20can%20write%20%24EPE%24%20as%20%0A%24%24%0A%5Chat%7BEPE%7D%20%3D%20E_X%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20L%5BG_k%2C%20%5Chat%7BG%7D(X)%5D%20Pr(G_k%20%7C%20X).%20%5Ctag%7B2.20%7D%0A%24%24%0Aand%20again%20it%20suffices%20to%20minimize%20EPE%20pointwise%3A%0A%24%24%0A%5Chat%7BG%7D(x)%20%3D%20%5Carg%5Cmin_%7Bg%20%5Cin%20G%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20L(g_k%2C%20g)%20%5CPr(g_k%20%7C%20X%20%3D%20x).%20%5Ctag%7B2.21%7D%0A%24%24%0AWith%20the%200%E2%80%931%20loss%20function%20this%20simplifies%20to%0A%24%24%0A%5Chat%7BG%7D(x)%20%3D%20%5Carg%5Cmin_%7Bg%20%5Cin%20G%7D%20%5B1%20-%20%5CPr(g%20%7C%20X%20%3D%20x)%5D%20%5Ctag%7B2.22%7D%0A%24%24%0Aor%20simply%0A%24%24%0A%5Chat%7BG%7D(X)%20%3D%20G_k%20%5Ctext%7B%20if%20%7D%20%5CPr(G_k%20%7C%20X%20%3D%20x)%20%3D%20%5Cmax_%7Bg%20%5Cin%20G%7D%20%5CPr(g%20%7C%20X%20%3D%20x).%20%5Ctag%7B2.23%7D%0A%24%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="51" data-gn-md="This%20reasonable%20solution%20is%20known%20as%20the%20Bayes%20classifier%2C%20and%20says%20that%20we%20classify%20to%20the%20most%20probable%20class%2C%20using%20the%20conditional%20(discrete)%20distribution%20%24%5CPr(G%20%7C%20X)%24.%20Figure%202.5%20shows%20the%20Bayes-optimal%20decision%20boundary%20for%20our%20simulation%20example.%20The%20error%20rate%20of%20the%20Bayes%20classifier%20is%20called%20the%20Bayes%20rate."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="52" data-gn-md="!%5B%5BPasted%20image%2020250825105806.png%5D%5D%0A**FIGURE%202.5.**%20The%20optimal%20Bayes%20decision%20boundary%20for%20the%20simulation%20example%20of%20Figures%202.1%2C%202.2%20and%202.3.%20Since%20the%20generating%20density%20is%20known%20for%20each%20class%2C%20this%20boundary%20can%20be%20calculated%20exactly%20(Exercise%202.2)."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="53" data-gn-md="Again%20we%20see%20that%20the%20%24k%24-nearest%20neighbor%20classifier%20directly%20approximates%20this%20solution%E2%80%94a%20majority%20vote%20in%20a%20nearest%20neighborhood%20amounts%20to%20exactly%20this%2C%20except%20that%20conditional%20probability%20at%20a%20point%20is%20relaxed%20to%20conditional%20probability%20within%20a%20neighborhood%20of%20a%20point%2C%20and%20probabilities%20are%20estimated%20by%20training-sample%20proportions."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="54" data-gn-md="Suppose%20for%20a%20two-class%20problem%20we%20had%20taken%20the%20dummy-variable%20approach%20and%20coded%20%24G%24%20via%20a%20binary%20%24Y%24%2C%20followed%20by%20squared%20error%20loss%20estimation.%20Then%20%24%5Chat%7Bf%7D(X)%20%3D%20E(Y%20%7C%20X)%20%3D%20Pr(G%20%3D%20G_1%20%7C%20X)%24%20if%20%24G_1%24%20corresponded%20to%20%24Y%20%3D%201%24.%20Likewise%20for%20a%20%24K%24-class%20problem%2C%20%24E(Y_k%20%7C%20X)%20%3D%20Pr(G%20%3D%20G_k%20%7C%20X)%24.%20This%20shows%20that%20our%20dummy-variable%20regression%20procedure%2C%20followed%20by%20classification%20to%20the%20largest%20fitted%20value%2C%20is%20another%20way%20of%20representing%20the%20Bayes%20classifier.%20Although%20this%20theory%20is%20exact%2C%20in%20practice%20problems%20can%20occur%2C%20depending%20on%20the%20regression%20model%20used.%20For%20example%2C%20when%20linear%20regression%20is%20used%2C%20%24%5Chat%7Bf%7D(X)%24%20need%20not%20be%20positive%2C%20and%20we%20might%20be%20suspicious%20about%20using%20it%20as%20an%20estimate%20of%20a%20probability.%20We%20will%20discuss%20a%20variety%20of%20approaches%20to%20modeling%20%24Pr(G%20%7C%20X)%24%20in%20Chapter%204."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="55" data-gn-md="%23%23%202.5%20Local%20Methods%20in%20High%20Dimensions"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="56" data-gn-md="We%20have%20examined%20two%20learning%20techniques%20for%20prediction%20so%20far%3A%20the%20stable%20but%20biased%20linear%20model%20and%20the%20less%20stable%20but%20apparently%20less%20biased%20class%20of%20%24k%24-nearest-neighbor%20estimates.%20It%20would%20seem%20that%20with%20a%20reasonably%20large%20set%20of%20training%20data%2C%20we%20could%20always%20approximate%20the%20theoretically%20optimal%20conditional%20expectation%20by%20%24k%24-nearest-neighbor%20averaging%2C%20since%20we%20should%20be%20able%20to%20find%20a%20fairly%20large%20neighborhood%20of%20observations%20close%20to%20any%20%24x%24%20and%20average%20them.%20This%20approach%20and%20our%20intuition%20breaks%20down%20in%20high%20dimensions%2C%20and%20the%20phenomenon%20is%20commonly%20referred%20to%20as%20the%20curse%20of%20dimensionality%20(Bellman%2C%201961).%20There%20are%20many%20manifestations%20of%20this%20problem%2C%20and%20we%20will%20examine%20a%20few%20here."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="57" data-gn-md="!%5B%5BPasted%20image%2020250825105951.png%5D%5D%0A**FIGURE%202.6.**%20The%20curse%20of%20dimensionality%20is%20well%20illustrated%20by%20a%20subcubical%20neighborhood%20for%20uniform%20data%20in%20a%20unit%20cube.%20The%20figure%20on%20the%20right%20shows%20the%20side-length%20of%20the%20subcube%20needed%20to%20capture%20a%20fraction%20%24r%24%20of%20the%20volume%20of%20the%20data%2C%20for%20different%20dimensions%20%24p%24.%20In%20ten%20dimensions%2C%20we%20need%20to%20cover%2080%25%20of%20the%20range%20of%20each%20coordinate%20to%20capture%2010%25%20of%20the%20data."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="58" data-gn-md="Consider%20the%20nearest-neighbor%20procedure%20for%20inputs%20uniformly%20distributed%20in%20a%20%24p%24-dimensional%20unit%20hypercube%2C%20as%20in%20Figure%202.6.%20Suppose%20we%20send%20out%20a%20hypercubical%20neighborhood%20about%20a%20target%20point%20to%20capture%20a%20fraction%20%24r%24%20of%20the%20observations.%20Since%20this%20corresponds%20to%20a%20fraction%20%24r%24%20of%20the%20unit%20volume%2C%20the%20expected%20edge%20length%20will%20be%20%24e_p(r)%20%3D%20r%5E%7B1%2Fp%7D%24.%20In%20ten%20dimensions%2C%20%24e_%7B10%7D(0.01)%20%3D%200.63%24%20and%20%24e_%7B10%7D(0.1)%20%3D%200.80%24%2C%20while%20the%20entire%20range%20for%20each%20input%20is%20only%201.0.%20So%20to%20capture%201%25%20or%2010%25%20of%20the%20data%20to%20form%20a%20local%20average%2C%20we%20must%20cover%2063%25%20or%2080%25%20of%20the%20range%20of%20each%20input%20variable.%20Such%20neighborhoods%20are%20no%20longer%20%E2%80%9Clocal.%E2%80%9D%20Reducing%20%24r%24%20dramatically%20does%20not%20help%20much%20either%2C%20since%20the%20fewer%20observations%20we%20average%2C%20the%20higher%20is%20the%20variance%20of%20our%20fit."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="59" data-gn-md="Another%20consequence%20of%20the%20sparse%20sampling%20in%20high%20dimensions%20is%20that%20all%20sample%20points%20are%20close%20to%20an%20edge%20of%20the%20sample.%20Consider%20%24N%24%20data%20points%20uniformly%20distributed%20in%20a%20%24p%24-dimensional%20unit%20ball%20centered%20at%20the%20origin.%20Suppose%20we%20consider%20a%20nearest-neighbor%20estimate%20at%20the%20origin.%20The%20median%20distance%20from%20the%20origin%20to%20the%20closest%20data%20point%20is%20given%20by%20the%20expression%0A%24%24%0Ad(p%2C%20N)%20%3D%20%5Cleft(1%20-%20%5Cfrac%7B1%7D%7B2%5E%7B1%2FN%7D%7D%5Cright)%5E%7B1%2Fp%7D%20%5Ctag%7B2.24%7D%0A%24%24%0A(Exercise%202.3).%20A%20more%20complicated%20expression%20exists%20for%20the%20mean%20distance%20to%20the%20closest%20point.%20For%20%24N%20%3D%20500%24%2C%20%24p%20%3D%2010%24%2C%20%24d(p%2C%20N)%20%5Capprox%200.52%24%2C%20more%20than%20halfway%20to%20the%20boundary.%20Hence%2C%20most%20data%20points%20are%20closer%20to%20the%20boundary%20of%20the%20sample%20space%20than%20to%20any%20other%20data%20point.%20The%20reason%20that%20this%20presents%20a%20problem%20is%20that%20prediction%20is%20much%20more%20difficult%20near%20the%20edges%20of%20the%20training%20sample.%20One%20must%20extrapolate%20from%20neighboring%20sample%20points%20rather%20than%20interpolate%20between%20them."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="60" data-gn-md="Another%20manifestation%20of%20the%20curse%20is%20that%20the%20sampling%20density%20is%20proportional%20to%20%24N%5E%7B1%2Fp%7D%24%2C%20where%20%24p%24%20is%20the%20dimension%20of%20the%20input%20space%20and%20%24N%24%20is%20the%20sample%20size.%20Thus%2C%20if%20%24N_1%20%3D%20100%24%20represents%20a%20dense%20sample%20for%20a%20single%20input%20problem%2C%20then%20%24N_%7B10%7D%20%3D%20100%5E%7B10%7D%24%20is%20the%20sample%20size%20required%20for%20the%20same%20sampling%20density%20with%2010%20inputs.%20Thus%2C%20in%20high%20dimensions%2C%20all%20feasible%20training%20samples%20sparsely%20populate%20the%20input%20space."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="61" data-gn-md="Let%20us%20construct%20another%20uniform%20example.%20Suppose%20we%20have%201000%20training%20examples%20%24x_i%24%20generated%20uniformly%20on%20%24%5B-1%2C%201%5D%24.%20Assume%20that%20the%20true%20relationship%20between%20%24X%24%20and%20%24Y%24%20is%20%0A%24%24%0AY%20%3D%20f(X)%20%3D%20e%5E%7B-8%20%5C%7CX%5C%7C%5E2%7D%2C%0A%24%24%0Awithout%20any%20measurement%20error.%20We%20use%20the%201-nearest-neighbor%20rule%20to%20predict%20%24y_0%24%20at%20the%20test%20point%20%24x_0%20%3D%200%24.%20Denote%20the%20training%20set%20by%20%24T%24.%20%20We%20can%20compute%20the%20expected%20prediction%20error%20at%20%24x_0%24%20for%20our%20procedure%2C%20averaging%20over%20all%20such%20samples%20of%20size%201000.%20Since%20the%20problem%20is%20deterministic%2C%20this%20is%20the%20mean%20squared%20error%20(MSE)%20for%20estimating%20%24f(0)%24%3A%0A%24%24%0AMSE(x_0)%20%3D%20E_T%5Bf(x_0)%20-%20%5Chat%7By%7D_0%5D%5E2%20%3D%20E_T%5B%5Chat%7By%7D_0%20-%20E_T(%5Chat%7By%7D_0)%5D%5E2%20%2B%20%5BE_T(%5Chat%7By%7D_0)%20-%20f(x_0)%5D%5E2%20%3D%20Var_T(%5Chat%7By%7D_0)%20%2B%20Bias%5E2(%5Chat%7By%7D_0).%20%5Cquad%20(2.25)%0A%24%24%0AFigure%202.7%20illustrates%20the%20setup.%20We%20have%20broken%20down%20the%20MSE%20into%20two%20components%20that%20will%20become%20familiar%20as%20we%20proceed%3A%20variance%20and%20squared%20bias.%20Such%20a%20decomposition%20is%20always%20possible%20and%20often%20useful%2C%20and%20is%20known%20as%20the%20bias%E2%80%93variance%20decomposition.%20Unless%20the%20nearest%20neighbor%20is%20at%200%2C%20%24%5Chat%7By%7D_0%24%20will%20be%20smaller%20than%20%24f(0)%24%20in%20this%20example%2C%20and%20so%20the%20average%20estimate%20will%20be%20biased%20downward.%20The%20variance%20is%20due%20to%20the%20sampling%20variance%20of%20the%201-nearest%20neighbor.%20In%20low%20dimensions%20and%20with%20%24N%20%3D%201000%24%2C%20the%20nearest%20neighbor%20is%20very%20close%20to%200%2C%20and%20so%20both%20the%20bias%20and%20variance%20are%20small.%20As%20the%20dimension%20increases%2C%20the%20nearest%20neighbor%20tends%20to%20stray%20further%20from%20the%20target%20point%2C%20and%20both%20bias%20and%20variance%20are%20incurred.%20By%20%24p%20%3D%2010%24%2C%20for%20more%20than%2099%25%20of%20the%20samples%20the%20nearest%20neighbor%20is%20a%20distance%20greater%20than%200.5%20from%20the%20origin.%20Thus%20as%20%24p%24%20increases%2C%20the%20estimate%20tends%20to%20be%200%20more%20often%20than%20not%2C%20and%20hence%20the%20MSE%20levels%20off%20at%201.0%2C%20as%20does%20the%20bias%2C%20and%20the%20variance%20starts%20dropping%20(an%20artifact%20of%20this%20example)."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="62" data-gn-md="!%5B%5BPasted%20image%2020250825110226.png%5D%5D%0A**FIGURE%202.7.**%20A%20simulation%20example%2C%20demonstrating%20the%20curse%20of%20dimensionality%20and%20its%20effect%20on%20MSE%2C%20bias%2C%20and%20variance.%20The%20input%20features%20are%20uniformly%20distributed%20in%20%24%5B-1%2C%201%5D%5Ep%24%20for%20%24p%20%3D%201%2C%20%5Cldots%2C%2010%24.%20The%20top%20left%20panel%20shows%20the%20target%20function%20(no%20noise)%20in%20%24%5Cmathbb%7BR%7D%5E2%24%3A%20%24f(X)%20%3D%20e%5E%7B-8%20%5C%7CX%5C%7C%5E2%7D%24%2C%20and%20demonstrates%20the%20error%20that%201-nearest%20neighbor%20makes%20in%20estimating%20%24f(0)%24.%20The%20training%20point%20is%20indicated%20by%20the%20blue%20tick%20mark.%20The%20top%20right%20panel%20illustrates%20why%20the%20radius%20of%20the%201-nearest%20neighborhood%20increases%20with%20dimension%20%24p%24.%20The%20lower%20left%20panel%20shows%20the%20average%20radius%20of%20the%201-nearest%20neighborhoods.%20The%20lower-right%20panel%20shows%20the%20MSE%2C%20squared%20bias%2C%20and%20variance%20curves%20as%20a%20function%20of%20dimension%20%24p%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="63" data-gn-md="Although%20this%20is%20a%20highly%20contrived%20example%2C%20similar%20phenomena%20occur%20more%20generally.%20The%20complexity%20of%20functions%20of%20many%20variables%20can%20grow%20exponentially%20with%20the%20dimension%2C%20and%20if%20we%20wish%20to%20be%20able%20to%20estimate%20such%20functions%20with%20the%20same%20accuracy%20as%20functions%20in%20low%20dimensions%2C%20then%20we%20need%20the%20size%20of%20our%20training%20set%20to%20grow%20exponentially%20as%20well.%20In%20this%20example%2C%20the%20function%20is%20a%20complex%20interaction%20of%20all%20%24p%24%20variables%20involved."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="64" data-gn-md="The%20dependence%20of%20the%20bias%20term%20on%20distance%20depends%20on%20the%20truth%2C%20and%20it%20need%20not%20always%20dominate%20with%201-nearest%20neighbor.%20For%20example%2C%20if%20the%20function%20always%20involves%20only%20a%20few%20dimensions%20as%20in%20Figure%202.8%2C%20then%20the%20variance%20can%20dominate%20instead."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="65" data-gn-md="!%5B%5BPasted%20image%2020250825110451.png%5D%5D%0A**Figure%202.8.**%20A%20simulation%20example%20with%20the%20same%20setup%20as%20in%20Figure%202.7.%20Here%20the%20function%20is%20constant%20in%20all%20but%20one%20dimension%3A%20%24F(X)%20%3D%20%5Cfrac%7B1%7D%7B2%7D(X_1%20%2B%201)%5E3%24.%20The%20variance%20dominates."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="66" data-gn-md="Suppose%2C%20on%20the%20other%20hand%2C%20that%20we%20know%20that%20the%20relationship%20between%20%24Y%24%20and%20%24X%24%20is%20linear%2C%0A%24%24%0AY%20%3D%20X%5ET%20%5Cbeta%20%2B%20%5Cepsilon%2C%20%5Cquad%20(2.26)%0A%24%24%0Awhere%20%24%5Cepsilon%20%5Csim%20N(0%2C%20%5Csigma%5E2)%24%20and%20we%20fit%20the%20model%20by%20least%20squares%20to%20the%20training%20data.%20For%20an%20arbitrary%20test%20point%20%24x_0%24%2C%20we%20have%20%24%5Chat%7By%7D_0%20%3D%20x%5ET%20%5Cbeta%24%2C%20which%20can%20be%20written%20as%20%24%5Chat%7By%7D_0%20%3D%20x%5ET%20%5Cbeta%20%2B%20%5Csum_%7Bi%3D1%7D%5EN%20%5Cell_i(x_0)%20%5Cepsilon_i%24%2C%20where%20%24%5Cell_i(x_0)%24%20is%20the%20%24i%24th%20element%20of%20%24X(X%5ETX)%5E%7B-1%7Dx_0%24.%20Since%20under%20this%20model%20the%20least%20squares%20estimates%20are%20unbiased%2C%20we%20find%20that%0A%24%24%0AEPE(x_0)%20%3D%20E_%7By_0%7Cx_0%7D%20E_T%20(y_0%20-%20%5Chat%7By%7D_0)%5E2%20%3D%20Var(y_0%7Cx_0)%20%2B%20E_T%20%5B%5Chat%7By%7D_0%20-%20E_T%20%5Chat%7By%7D_0%5D%5E2%20%2B%20%5BE_T%20%5Chat%7By%7D_0%20-%20x%5ET%20%5Cbeta%5D%5E2%0A%24%24%0A%24%24%0A%3D%20Var(y_0%7Cx_0)%20%2B%20Var_T(%5Chat%7By%7D_0)%20%2B%20Bias(%5Chat%7By%7D_0)%5E2%20%5Ctag%7B2.27%7D%0A%24%24%0AHere%20we%20have%20incurred%20an%20additional%20variance%20%24%5Csigma%5E2%24%20in%20the%20prediction%20error%2C%20since%20our%20target%20is%20not%20deterministic.%20There%20is%20no%20bias%2C%20and%20the%20variance%20depends%20on%20%24x_0%24.%20If%20%24N%24%20is%20large%20and%20%24T%24%20were%20selected%20at%20random%2C%20and%20assuming%20%24E(X)%20%3D%200%24%2C%20then"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="67" data-gn-md="%24%24%0AE_%7Bx_0%7D%20EPE(x_0)%20%5Csim%20E_%7Bx_0%7D%20%5Ctext%7Btrace%7D%5BCov(X)%20Cov(x_0)%5D%20%5Csigma%5E2%2FN%20%2B%20%5Csigma%5E2%20%5Ctag%7B2.28%7D%0A%24%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="68" data-gn-md="Here%20we%20see%20that%20the%20expected%20EPE%20increases%20linearly%20as%20a%20function%20of%20%24p%24%2C%20with%20slope%20%24%5Csigma%5E2%2FN%24.%20If%20%24N%24%20is%20large%20and%2For%20%24%5Csigma%5E2%24%20is%20small%2C%20this%20growth%20in%20variance%20is%20negligible%20(0%20in%20the%20deterministic%20case).%20By%20imposing%20some%20heavy%20restrictions%20on%20the%20class%20of%20models%20being%20fitted%2C%20we%20have%20avoided%20the%20curse%20of%20dimensionality.%20Some%20of%20the%20technical%20details%20in%20(2.27)%20and%20(2.28)%20are%20derived%20in%20Exercise%202.5."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="69" data-gn-md="Figure%202.9%20compares%201-nearest%20neighbor%20vs.%20least%20squares%20in%20two%20situations%2C%20both%20of%20which%20have%20the%20form%20%24Y%20%3D%20f(X)%20%2B%20%5Cepsilon%24%2C%20%24X%24%20uniform%20as%20before%2C%20and%20%24%5Cepsilon%20%5Csim%20N(0%2C%201)%24.%20The%20sample%20size%20is%20%24N%20%3D%20500%24.%20For%20the%20orange%20curve%2C%20%24f(x)%24%20is%20linear%20in%20the%20first%20coordinate%2C%20for%20the%20blue%20curve%2C%20cubic%20as%20in%20Figure%202.8.%20%20Shown%20is%20the%20relative%20EPE%20of%201-nearest%20neighbor%20to%20least%20squares%2C%20which%20appears%20to%20start%20at%20around%202%20for%20the%20linear%20case.%20Least%20squares%20is%20unbiased%20in%20this%20case%2C%20and%20as%20discussed%20above%2C%20the%20EPE%20is%20slightly%20above%20%24%5Csigma%5E2%20%3D%201%24.%20The%20EPE%20for%201-nearest%20neighbor%20is%20always%20above%202%2C%20since%20the%20variance%20of%20%24%5Chat%7Bf%7D(x_0)%24%20in%20this%20case%20is%20at%20least%20%24%5Csigma%5E2%24%2C%20and%20the%20ratio%20increases%20with%20dimension%20as%20the%20nearest%20neighbor%20strays%20from%20the%20target%20point.%20For%20the%20cubic%20case%2C%20least%20squares%20is%20biased%2C%20which%20moderates%20the%20ratio.%20Clearly%2C%20we%20could%20manufacture%20examples%20where%20the%20bias%20of%20least%20squares%20would%20dominate%20the%20variance%2C%20and%20the%201-nearest%20neighbor%20would%20come%20out%20the%20winner."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="70" data-gn-md="!%5B%5BPasted%20image%2020250825111744.png%5D%5D%0A**Figure%202.9.**%20The%20curves%20show%20the%20expected%20prediction%20error%20(at%20%24x_0%20%3D%200%24)%20for%201-nearest%20neighbor%20relative%20to%20least%20squares%20for%20the%20model%20%24Y%20%3D%20f(X)%20%2B%20%5Cepsilon%24.%20For%20the%20orange%20curve%2C%20%24f(x)%20%3D%20x_1%24%2C%20while%20for%20the%20blue%20curve%2C%20%24f(x)%20%3D%20%5Cfrac%7B1%7D%7B3%7D(x_1%20%2B%201)%5E3%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="71" data-gn-md="By%20relying%20on%20rigid%20assumptions%2C%20the%20linear%20model%20has%20no%20bias%20at%20all%20and%20negligible%20variance%2C%20while%20the%20error%20in%201-nearest%20neighbor%20is%20substantially%20larger.%20However%2C%20if%20the%20assumptions%20are%20wrong%2C%20all%20bets%20are%20off%20and%20the%201-nearest%20neighbor%20may%20dominate.%20We%20will%20see%20that%20there%20is%20a%20whole%20spectrum%20of%20models%20between%20the%20rigid%20linear%20models%20and%20the%20extremely%20flexible%201-nearest-neighbor%20models%2C%20each%20with%20their%20own%20assumptions%20and%20biases%2C%20which%20have%20been%20proposed%20specifically%20to%20avoid%20the%20exponential%20growth%20in%20complexity%20of%20functions%20in%20high%20dimensions%20by%20drawing%20heavily%20on%20these%20assumptions."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="72" data-gn-md="Before%20we%20delve%20more%20deeply%2C%20let%20us%20elaborate%20a%20bit%20on%20the%20concept%20of%20statistical%20models%20and%20see%20how%20they%20fit%20into%20the%20prediction%20framework.%0A%23%23%202.6%20Statistical%20Models%2C%20Supervised%20Learning%20and%20Function%20Approximation"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="73" data-gn-md="Our%20goal%20is%20to%20find%20a%20useful%20approximation%20%24%5Chat%7Bf%7D(x)%24%20to%20the%20function%20%24f(x)%24%20that%20underlies%20the%20predictive%20relationship%20between%20the%20inputs%20and%20outputs.%20In%20the%20theoretical%20setting%20of%20Section%202.4%2C%20we%20saw%20that%20squared%20error%20loss%20led%20us%20to%20the%20regression%20function%20%24f(x)%20%3D%20E(Y%20%7C%20X%20%3D%20x)%24%20for%20a%20quantitative%20response.%20The%20class%20of%20nearest-neighbor%20methods%20can%20be%20viewed%20as%20direct%20estimates%20of%20this%20conditional%20expectation%2C%20but%20we%20have%20seen%20that%20they%20can%20fail%20in%20at%20least%20two%20ways%3A%0A-%20If%20the%20dimension%20of%20the%20input%20space%20is%20high%2C%20the%20nearest%20neighbors%20need%20not%20be%20close%20to%20the%20target%20point%2C%20and%20can%20result%20in%20large%20errors.%0A-%20If%20special%20structure%20is%20known%20to%20exist%2C%20this%20can%20be%20used%20to%20reduce%20both%20the%20bias%20and%20the%20variance%20of%20the%20estimates.%0AWe%20anticipate%20using%20other%20classes%20of%20models%20for%20%24f(x)%24%2C%20in%20many%20cases%20specifically%20designed%20to%20overcome%20the%20dimensionality%20problems%2C%20and%20here%20we%20discuss%20a%20framework%20for%20incorporating%20them%20into%20the%20prediction%20problem."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="74" data-gn-md="%23%23%23%202.6.1%20A%20Statistical%20Model%20for%20the%20Joint%20Distribution%20%24Pr(X%2C%20Y)%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="75" data-gn-md="Suppose%20in%20fact%20that%20our%20data%20arose%20from%20a%20statistical%20model%0A%24%24%20Y%20%3D%20f(X)%20%2B%20%5Cepsilon%2C%20%5Cquad%20(2.29)%20%24%24%0Awhere%20the%20random%20error%20%24%5Cepsilon%24%20has%20%24E(%5Cepsilon)%20%3D%200%24%20and%20is%20independent%20of%20%24X%24.%20Note%20that%20for%20this%20model%2C%20%24f(x)%20%3D%20E(Y%20%7C%20X%20%3D%20x)%24%2C%20and%20in%20fact%20the%20conditional%20distribution%20%24Pr(Y%20%7C%20X)%24%20depends%20on%20%24X%24%20only%20through%20the%20conditional%20mean%20%24f(x)%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="76" data-gn-md="The%20additive%20error%20model%20is%20a%20useful%20approximation%20to%20the%20truth.%20For%20most%20systems%2C%20the%20input%E2%80%93output%20pairs%20%24(X%2C%20Y)%24%20will%20not%20have%20a%20deterministic%20relationship%20%24Y%20%3D%20f(X)%24.%20Generally%2C%20there%20will%20be%20other%20unmeasured%20variables%20that%20also%20contribute%20to%20%24Y%24%2C%20including%20measurement%20error.%20The%20additive%20model%20assumes%20that%20we%20can%20capture%20all%20these%20departures%20from%20a%20deterministic%20relationship%20via%20the%20error%20%24%5Cepsilon%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="77" data-gn-md="For%20some%20problems%2C%20a%20deterministic%20relationship%20does%20hold.%20Many%20of%20the%20classification%20problems%20studied%20in%20machine%20learning%20are%20of%20this%20form%2C%20where%20the%20response%20surface%20can%20be%20thought%20of%20as%20a%20colored%20map%20defined%20in%20%24%5Cmathbb%7BR%7D%5Ep%24.%20The%20training%20data%20consist%20of%20colored%20examples%20from%20the%20map%20%24%5C%7Bx_i%2C%20g_i%5C%7D%24%2C%20and%20the%20goal%20is%20to%20be%20able%20to%20color%20any%20point.%20Here%20the%20function%20is%20deterministic%2C%20and%20the%20randomness%20enters%20through%20the%20%24x%24%20location%20of%20the%20training%20points.%20For%20the%20moment%2C%20we%20will%20not%20pursue%20such%20problems%2C%20but%20we%20will%20see%20that%20they%20can%20be%20handled%20by%20techniques%20appropriate%20for%20the%20error-based%20models."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="78" data-gn-md="The%20assumption%20in%20(2.29)%20that%20the%20errors%20are%20independent%20and%20identically%20distributed%20is%20not%20strictly%20necessary%2C%20but%20seems%20to%20be%20at%20the%20back%20of%20our%20mind%20when%20we%20average%20squared%20errors%20uniformly%20in%20our%20EPE%20criterion.%20With%20such%20a%20model%2C%20it%20becomes%20natural%20to%20use%20least%20squares%20as%20a%20data%20criterion%20for%20model%20estimation%20as%20in%20(2.1).%20Simple%20modifications%20can%20be%20made%20to%20avoid%20the%20independence%20assumption%3B%20for%20example%2C%20we%20can%20have%20%2C%20and%20now%20both%20the%20mean%20and%20variance%20depend%20on%20.%20In%20general%2C%20the%20conditional%20distribution%20%20can%20depend%20on%20%20in%20complicated%20ways%2C%20but%20the%20additive%20error%20model%20precludes%20these."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="79" data-gn-md="So%20far%2C%20we%20have%20concentrated%20on%20the%20quantitative%20response.%20Additive%20error%20models%20are%20typically%20not%20used%20for%20qualitative%20outputs%20%24G%24%3B%20in%20this%20case%2C%20the%20target%20function%20%24p(X)%24%20is%20the%20conditional%20density%20%24%5Ctext%7BPr%7D(G%20%7C%20X)%24%2C%20and%20this%20is%20modeled%20directly.%20For%20example%2C%20for%20two-class%20data%2C%20it%20is%20often%20reasonable%20to%20assume%20that%20the%20data%20arise%20from%20independent%20binary%20trials%2C%20with%20the%20probability%20of%20one%20particular%20outcome%20being%20%24p(X)%24%2C%20and%20the%20other%20%241%20-%20p(X)%24.%20Thus%20if%20%24Y%24%20is%20the%200%E2%80%931%20coded%20version%20of%20%24G%24%2C%20then%20%24E(Y%20%7C%20X%20%3D%20x)%20%3D%20p(x)%24%2C%20but%20the%20variance%20depends%20on%20%24x%24%20as%20well%3A%20%24%5Ctext%7BVar%7D(Y%20%7C%20X%20%3D%20x)%20%3D%20p(x)%5B1%20-%20p(x)%5D%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="80" data-gn-md="%23%23%202.6.2%20Supervised%20Learning"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="81" data-gn-md="Before%20we%20launch%20into%20more%20statistically%20oriented%20jargon%2C%20we%20present%20the%20function-fitting%20paradigm%20from%20a%20machine%20learning%20point%20of%20view.%20Suppose%20for%20simplicity%20that%20the%20errors%20are%20additive%20and%20that%20the%20model%20%24Y%20%3D%20f(X)%20%2B%20%5Cepsilon%24%20is%20a%20reasonable%20assumption.%20Supervised%20learning%20attempts%20to%20learn%20%24f%24%20by%20example%20through%20a%20teacher.%20One%20observes%20the%20system%20under%20study%2C%20both%20the%20inputs%20and%20outputs%2C%20and%20assembles%20a%20training%20set%20of%20observations%20%24T%20%3D%20(x_i%2C%20y_i)%2C%20i%20%3D%201%2C%20%5Cldots%2C%20N%24.%20The%20observed%20input%20values%20to%20the%20system%20%24x_i%24%20are%20also%20fed%20into%20an%20artificial%20system%2C%20known%20as%20a%20learning%20algorithm%20(usually%20a%20computer%20program)%2C%20which%20also%20produces%20outputs%20%24f(x_i)%24%20in%20response%20to%20the%20inputs.%20The%20learning%20algorithm%20has%20the%20property%20that%20it%20can%20modify%20its%20input%2Foutput%20relationship%20%24f%24%20in%20response%20to%20differences%20%24y_i%20-%20f(x_i)%24%20between%20the%20original%20and%20generated%20outputs.%20This%20process%20is%20known%20as%20learning%20by%20example.%20Upon%20completion%20of%20the%20learning%20process%2C%20the%20hope%20is%20that%20the%20artificial%20and%20real%20outputs%20will%20be%20close%20enough%20to%20be%20useful%20for%20all%20sets%20of%20inputs%20likely%20to%20be%20encountered%20in%20practice."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="82" data-gn-md="%23%23%202.6.3%20Function%20Approximation"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="83" data-gn-md="The%20learning%20paradigm%20of%20the%20previous%20section%20has%20been%20the%20motivation%20for%20research%20into%20the%20supervised%20learning%20problem%20in%20the%20fields%20of%20machine%20learning%20(with%20analogies%20to%20human%20reasoning)%20and%20neural%20networks%20(with%20biological%20analogies%20to%20the%20brain).%20The%20approach%20taken%20in%20applied%20mathematics%20and%20statistics%20has%20been%20from%20the%20perspective%20of%20function%20approximation%20and%20estimation.%20Here%20the%20data%20pairs%20%24%5C%7Bx_i%2C%20y_i%5C%7D%24%20are%20viewed%20as%20points%20in%20a%20%24(p%20%2B%201)%24-dimensional%20Euclidean%20space.%20The%20function%20%24f(x)%24%20has%20domain%20equal%20to%20the%20%24p%24-dimensional%20input%20subspace%20and%20is%20related%20to%20the%20data%20via%20a%20model%20such%20as%20%24y_i%20%3D%20f(x_i)%20%2B%20%5Cepsilon_i%24.%20For%20convenience%20in%20this%20chapter%20we%20will%20assume%20the%20domain%20is%20%24%5Cmathbb%7BR%7D%5Ep%24%2C%20a%20%24p%24-dimensional%20Euclidean%20space%2C%20although%20in%20general%20the%20inputs%20can%20be%20of%20mixed%20type.%20The%20goal%20is%20to%20obtain%20a%20useful%20approximation%20to%20%24f(x)%24%20for%20all%20%24x%24%20in%20some%20region%20of%20%24%5Cmathbb%7BR%7D%5Ep%24%2C%20given%20the%20representations%20in%20%24T%24.%20Although%20somewhat%20less%20glamorous%20than%20the%20learning%20paradigm%2C%20treating%20supervised%20learning%20as%20a%20problem%20in%20function%20approximation%20encourages%20the%20geometrical%20concepts%20of%20Euclidean%20spaces%20and%20mathematical%20concepts%20of%20probabilistic%20inference%20to%20be%20applied%20to%20the%20problem.%20This%20is%20the%20approach%20taken%20in%20this%20book."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="84" data-gn-md="Many%20of%20the%20approximations%20we%20will%20encounter%20have%20associated%20a%20set%20of%20parameters%20%24%5Ctheta%24%20that%20can%20be%20modified%20to%20suit%20the%20data%20at%20hand.%20For%20example%2C%20the%20linear%20model%20%24f(x)%20%3D%20x%5ET%20%5Cbeta%24%20has%20%24%5Ctheta%20%3D%20%5Cbeta%24.%20Another%20class%20of%20useful%20approximators%20can%20be%20expressed%20as%20linear%20basis%20expansions%3A%0A%24%24%0Af_%5Ctheta(x)%20%3D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20h_k(x)%20%5Ctheta_k%2C%20%5Ctag%7B2.30%7D%0A%24%24%0Awhere%20the%20%24h_k%24%20are%20a%20suitable%20set%20of%20functions%20or%20transformations%20of%20the%20input%20vector%20%24x%24.%20Traditional%20examples%20are%20polynomial%20and%20trigonometric%20expansions%2C%20where%20for%20example%20%24h_k%24%20might%20be%20%24x_1%5E2%24%2C%20%24x_1%20x_2%24%2C%20%24%5Ccos(x_1)%24%2C%20and%20so%20on.%20We%20also%20encounter%20nonlinear%20expansions%2C%20such%20as%20the%20sigmoid%20transformation%20common%20to%20neural%20network%20models%3A%0A%24%24%0Ah_k(x)%20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20%5Cexp(-x%5ET%20%5Cbeta_k)%7D.%20%5Ctag%7B2.31%7D%0A%24%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="85" data-gn-md="We%20can%20use%20least%20squares%20to%20estimate%20the%20parameters%20%24%5Ctheta%24%20in%20%24f_%5Ctheta%24%20as%20we%20did%20for%20the%20linear%20model%2C%20by%20minimizing%20the%20residual%20sum-of-squares%3A%0A%24%24%0ARSS(%5Ctheta)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20(y_i%20-%20f_%5Ctheta(x_i))%5E2%20%5Ctag%7B2.32%7D%0A%24%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="86" data-gn-md="as%20a%20function%20of%20%24%5Ctheta%24.%20This%20seems%20a%20reasonable%20criterion%20for%20an%20additive%20error%20model.%20In%20terms%20of%20function%20approximation%2C%20we%20imagine%20our%20parameterized%20function%20as%20a%20surface%20in%20%24p%20%2B%201%24%20space%2C%20and%20what%20we%20observe%20are%20noisy%20realizations%20from%20it.%20This%20is%20easy%20to%20visualize%20when%20%24p%20%3D%202%24%20and%20the%20vertical%20coordinate%20is%20the%20output%20%24y%24%2C%20as%20in%20Figure%202.10.%20The%20noise%20is%20in%20the%20output%20coordinate%2C%20so%20we%20find%20the%20set%20of%20parameters%20such%20that%20the%20fitted%20surface%20gets%20as%20close%20to%20the%20observed%20points%20as%20possible%2C%20where%20close%20is%20measured%20by%20the%20sum%20of%20squared%20vertical%20errors%20in%20%24RSS(%5Ctheta)%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="87" data-gn-md="For%20the%20linear%20model%20we%20get%20a%20simple%20closed%20form%20solution%20to%20the%20minimization%20problem.%20This%20is%20also%20true%20for%20the%20basis%20function%20methods%2C%20if%20the%20basis%20functions%20themselves%20do%20not%20have%20any%20hidden%20parameters.%20Otherwise%20the%20solution%20requires%20either%20iterative%20methods%20or%20numerical%20optimization."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="88" data-gn-md="!%5B%5BPasted%20image%2020250825112141.png%5D%5D%0A**FIGURE%202.10.**%20Least%20squares%20fitting%20of%20a%20function%20of%20two%20inputs.%20The%20parameters%20of%20%24f_%5Ctheta(x)%24%20are%20chosen%20so%20as%20to%20minimize%20the%20sum-of-squared%20vertical%20errors."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="89" data-gn-md="While%20least%20squares%20is%20generally%20very%20convenient%2C%20it%20is%20not%20the%20only%20criterion%20used%20and%20in%20some%20cases%20would%20not%20make%20much%20sense.%20A%20more%20general%20principle%20for%20estimation%20is%20maximum%20likelihood%20estimation.%20Suppose%20we%20have%20a%20random%20sample%20%24y_i%24%2C%20%24i%20%3D%201%2C%20%5Cldots%2C%20N%24%20from%20a%20density%20%24P_%5Ctheta(y)%24%20indexed%20by%20some%20parameters%20%24%5Ctheta%24.%20The%20log-probability%20of%20the%20observed%20sample%20is%20%0A%24%24%0AL(%5Ctheta)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Clog%20P_%5Ctheta(y_i).%20%5Ctag%7B2.33%7D%0A%24%24%0AThe%20principle%20of%20maximum%20likelihood%20assumes%20that%20the%20most%20reasonable%20values%20for%20%24%5Ctheta%24%20are%20those%20for%20which%20the%20probability%20of%20the%20observed%20sample%20is%20largest.%20Least%20squares%20for%20the%20additive%20error%20model%20%24Y%20%3D%20f_%5Ctheta(X)%20%2B%20%5Cepsilon%24%2C%20with%20%24%5Cepsilon%20%5Csim%20N(0%2C%20%5Csigma%5E2)%24%2C%20is%20equivalent%20to%20maximum%20likelihood%20using%20the%20conditional%20likelihood%20%0A%24%24%0AP(Y%20%7C%20X%2C%20%5Ctheta)%20%3D%20N(f_%5Ctheta(X)%2C%20%5Csigma%5E2).%20%5Ctag%7B2.34%7D%0A%24%24%0ASo%20although%20the%20additional%20assumption%20of%20normality%20seems%20more%20restrictive%2C%20the%20results%20are%20the%20same.%20The%20log-likelihood%20of%20the%20data%20is%20%0A%24%24%0AL(%5Ctheta)%20%3D%20-%5Cfrac%7BN%7D%7B2%7D%20%5Clog(2%5Cpi)%20-%20N%20%5Clog%20%5Csigma%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20(y_i%20-%20f_%5Ctheta(x_i))%5E2%2C%20%5Ctag%7B2.35%7D%0A%24%24%0Aand%20the%20only%20term%20involving%20%24%5Ctheta%24%20is%20the%20last%2C%20which%20is%20RSS(%24%5Ctheta%24)%20up%20to%20a%20scalar%20negative%20multiplier."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="90" data-gn-md="A%20more%20interesting%20example%20is%20the%20multinomial%20likelihood%20for%20the%20regression%20function%20%24P(G%20%7C%20X)%24%20for%20a%20qualitative%20output%20%24G%24.%20Suppose%20we%20have%20a%20model%20%24P(G%20%3D%20G_k%20%7C%20X%20%3D%20x)%20%3D%20p_%7Bk%2C%5Ctheta%7D(x)%2C%20%5Cquad%20k%20%3D%201%2C%20%5Cldots%2C%20K%24%20for%20the%20conditional%20probability%20of%20each%20class%20given%20%24X%24%2C%20indexed%20by%20the%20parameter%20vector%20%24%5Ctheta%24.%20Then%20the%20log-likelihood%20(also%20referred%20to%20as%20the%20cross-entropy)%20is%20%0A%24%24%0AL(%5Ctheta)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Clog%20p_%7Bg_i%2C%20%5Ctheta%7D(x_i)%2C%20%5Ctag%7B2.36%7D%0A%24%24%0Aand%20when%20maximized%20it%20delivers%20values%20of%20%24%5Ctheta%24%20that%20best%20conform%20with%20the%20data%20in%20this%20likelihood%20sense."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="91" data-gn-md="%23%23%202.7%20Structured%20Regression%20Models"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="92" data-gn-md="We%20have%20seen%20that%20although%20nearest-neighbor%20and%20other%20local%20methods%20focus%20directly%20on%20estimating%20the%20function%20at%20a%20point%2C%20they%20face%20problems%20in%20high%20dimensions.%20They%20may%20also%20be%20inappropriate%20even%20in%20low%20dimensions%20in%20cases%20where%20more%20structured%20approaches%20can%20make%20more%20efficient%20use%20of%20the%20data.%20This%20section%20introduces%20classes%20of%20such%20structured%20approaches.%20Before%20we%20proceed%2C%20though%2C%20we%20discuss%20further%20the%20need%20for%20such%20classes."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="93" data-gn-md="%23%23%23%202.7.1%20Difficulty%20of%20the%20Problem"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="94" data-gn-md="Consider%20the%20RSS%20criterion%20for%20an%20arbitrary%20function%20%24f%24%2C%0A%24%24%0ARSS(f)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20(y_i%20-%20f(x_i))%5E2.%20%5Ctag%7B2.37%7D%0A%24%24%0AMinimizing%20(2.37)%20leads%20to%20infinitely%20many%20solutions%3A%20any%20function%20%24f%24%20passing%20through%20the%20training%20points%20%24(x_i%2C%20y_i)%24%20is%20a%20solution.%20Any%20particular%20solution%20chosen%20might%20be%20a%20poor%20predictor%20at%20test%20points%20different%20from%20the%20training%20points.%20If%20there%20are%20multiple%20observation%20pairs%20%24(x_i%2C%20y_%7Bi%5Cell%7D)%24%2C%20%24%5Cell%20%3D%201%2C%20%5Cldots%2C%20N_i%24%20at%20each%20value%20of%20%24x_i%24%2C%20the%20risk%20is%20limited.%20In%20this%20case%2C%20the%20solutions%20pass%20through%20the%20average%20values%20of%20the%20%24y_%7Bi%5Cell%7D%24%20at%20each%20%24x_i%24%3B%20see%20Exercise%202.6.%20The%20situation%20is%20similar%20to%20the%20one%20we%20have%20already%20visited%20in%20Section%202.4%3B%20indeed%2C%20(2.37)%20is%20the%20finite%20sample%20version%20of%20(2.11)%20on%20page%2018.%20If%20the%20sample%20size%20%24N%24%20were%20sufficiently%20large%20such%20that%20repeats%20were%20guaranteed%20and%20densely%20arranged%2C%20it%20would%20seem%20that%20these%20solutions%20might%20all%20tend%20to%20the%20limiting%20conditional%20expectation."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="95" data-gn-md="In%20order%20to%20obtain%20useful%20results%20for%20finite%20%24N%24%2C%20we%20must%20restrict%20the%20eligible%20solutions%20to%20(2.37)%20to%20a%20smaller%20set%20of%20functions.%20How%20to%20decide%20on%20the%20nature%20of%20the%20restrictions%20is%20based%20on%20considerations%20outside%20of%20the%20data.%20These%20restrictions%20are%20sometimes%20encoded%20via%20the%20parametric%20representation%20of%20%24f_%5Ctheta%24%2C%20or%20may%20be%20built%20into%20the%20learning%20method%20itself%2C%20either%20implicitly%20or%20explicitly.%20These%20restricted%20classes%20of%20solutions%20are%20the%20major%20topic%20of%20this%20book.%20One%20thing%20should%20be%20clear%2C%20though.%20Any%20restrictions%20imposed%20on%20%24f%24%20that%20lead%20to%20a%20unique%20solution%20to%20(2.37)%20do%20not%20really%20remove%20the%20ambiguity%20caused%20by%20the%20multiplicity%20of%20solutions.%20%20There%20are%20infinitely%20many%20possible%20restrictions%2C%20each%20leading%20to%20a%20unique%20solution%2C%20so%20the%20ambiguity%20has%20simply%20been%20transferred%20to%20the%20choice%20of%20a%20constraint."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="96" data-gn-md="In%20general%2C%20the%20constraints%20imposed%20by%20most%20learning%20methods%20can%20be%20described%20as%20complexity%20restrictions%20of%20one%20kind%20or%20another.%20This%20usually%20means%20some%20kind%20of%20regular%20behavior%20in%20small%20neighborhoods%20of%20the%20input%20space.%20That%20is%2C%20for%20all%20input%20points%20%24x%24%20sufficiently%20close%20to%20each%20other%20in%20some%20metric%2C%20%24%5Chat%7Bf%7D%24%20exhibits%20some%20special%20structure%20such%20as%20nearly%20constant%2C%20linear%2C%20or%20low-order%20polynomial%20behavior.%20The%20estimator%20is%20then%20obtained%20by%20averaging%20or%20polynomial%20fitting%20in%20that%20neighborhood."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="97" data-gn-md="The%20strength%20of%20the%20constraint%20is%20dictated%20by%20the%20neighborhood%20size.%20The%20larger%20the%20size%20of%20the%20neighborhood%2C%20the%20stronger%20the%20constraint%2C%20and%20the%20more%20sensitive%20the%20solution%20is%20to%20the%20particular%20choice%20of%20constraint.%20For%20example%2C%20local%20constant%20fits%20in%20infinitesimally%20small%20neighborhoods%20is%20no%20constraint%20at%20all%3B%20local%20linear%20fits%20in%20very%20large%20neighborhoods%20is%20almost%20a%20globally%20linear%20model%2C%20and%20is%20very%20restrictive."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="98" data-gn-md="The%20nature%20of%20the%20constraint%20depends%20on%20the%20metric%20used.%20Some%20methods%2C%20such%20as%20kernel%20and%20local%20regression%20and%20tree-based%20methods%2C%20directly%20specify%20the%20metric%20and%20size%20of%20the%20neighborhood.%20The%20nearest-neighbor%20methods%20discussed%20so%20far%20are%20based%20on%20the%20assumption%20that%20locally%20the%20function%20is%20constant%3B%20close%20to%20a%20target%20input%20%24x_0%24%2C%20the%20function%20does%20not%20change%20much%2C%20and%20so%20close%20outputs%20can%20be%20averaged%20to%20produce%20%24%5Chat%7Bf%7D(x_0)%24.%20Other%20methods%20such%20as%20splines%2C%20neural%20networks%2C%20and%20basis-function%20methods%20implicitly%20define%20neighborhoods%20of%20local%20behavior.%20In%20Section%205.4.1%20we%20discuss%20the%20concept%20of%20an%20equivalent%20kernel%20(see%20Figure%205.8%20on%20page%20157)%2C%20which%20describes%20this%20local%20dependence%20for%20any%20method%20linear%20in%20the%20outputs.%20These%20equivalent%20kernels%20in%20many%20cases%20look%20just%20like%20the%20explicitly%20defined%20weighting%20kernels%20discussed%20above%E2%80%94peaked%20at%20the%20target%20point%20and%20falling%20away%20smoothly%20away%20from%20it."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="99" data-gn-md="One%20fact%20should%20be%20clear%20by%20now.%20Any%20method%20that%20attempts%20to%20produce%20locally%20varying%20functions%20in%20small%20isotropic%20neighborhoods%20will%20run%20into%20problems%20in%20high%20dimensions%E2%80%94again%20the%20curse%20of%20dimensionality.%20And%20conversely%2C%20all%20methods%20that%20overcome%20the%20dimensionality%20problems%20have%20an%20associated%E2%80%94and%20often%20implicit%20or%20adaptive%E2%80%94metric%20for%20measuring%20neighborhoods%2C%20which%20basically%20does%20not%20allow%20the%20neighborhood%20to%20be%20simultaneously%20small%20in%20all%20directions."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="100" data-gn-md="%23%23%202.8%20Classes%20of%20Restricted%20Estimators"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="101" data-gn-md="The%20variety%20of%20nonparametric%20regression%20techniques%20or%20learning%20methods%20fall%20into%20a%20number%20of%20different%20classes%20depending%20on%20the%20nature%20of%20the%20restrictions%20imposed.%20These%20classes%20are%20not%20distinct%2C%20and%20indeed%20some%20methods%20fall%20in%20several%20classes.%20Here%20we%20give%20a%20brief%20summary%2C%20since%20detailed%20descriptions%20are%20given%20in%20later%20chapters.%20Each%20of%20the%20classes%20has%20associated%20with%20it%20one%20or%20more%20parameters%2C%20sometimes%20appropriately%20called%20smoothing%20parameters%2C%20that%20control%20the%20effective%20size%20of%20the%20local%20neighborhood.%20Here%20we%20describe%20three%20broad%20classes."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="102" data-gn-md="%23%23%202.8.1%20Roughness%20Penalty%20and%20Bayesian%20Methods"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="103" data-gn-md="Here%20the%20class%20of%20functions%20is%20controlled%20by%20explicitly%20penalizing%20%24RSS(f)%24%20with%20a%20roughness%20penalty%0A%24%24%0APRSS(f%3B%20%5Clambda)%20%3D%20RSS(f)%20%2B%20%5Clambda%20J(f).%20%5Ctag%7B2.38%7D%0A%24%24%0AThe%20user-selected%20functional%20%24J(f)%24%20will%20be%20large%20for%20functions%20%24f%24%20that%20vary%20too%20rapidly%20over%20small%20regions%20of%20input%20space.%20For%20example%2C%20the%20popular%20cubic%20smoothing%20spline%20for%20one-dimensional%20inputs%20is%20the%20solution%20to%20the%20penalized%20least-squares%20criterion%0A%24%24%0APRSS(f%3B%20%5Clambda)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20(y_i%20-%20f(x_i))%5E2%20%2B%20%5Clambda%20%5Cint%20%5Bf''(x)%5D%5E2%20dx.%20%5Ctag%7B2.39%7D%0A%24%24%0AThe%20roughness%20penalty%20here%20controls%20large%20values%20of%20the%20second%20derivative%20of%20%24f%24%2C%20and%20the%20amount%20of%20penalty%20is%20dictated%20by%20%24%5Clambda%20%5Cgeq%200%24.%20For%20%24%5Clambda%20%3D%200%24%20no%20penalty%20is%20imposed%2C%20and%20any%20interpolating%20function%20will%20do%2C%20while%20for%20%24%5Clambda%20%3D%20%5Cinfty%24%20only%20functions%20linear%20in%20%24x%24%20are%20permitted."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="104" data-gn-md="Penalty%20functionals%20%24J%24%20can%20be%20constructed%20for%20functions%20in%20any%20dimension%2C%20and%20special%20versions%20can%20be%20created%20to%20impose%20special%20structure.%20For%20example%2C%20additive%20penalties%20%24J(f)%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7D%20J(f_j)%24%20are%20used%20in%20conjunction%20with%20additive%20functions%20%24f(X)%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7D%20f_j(X_j)%24%20to%20create%20additive%20models%20with%20smooth%20coordinate%20functions.%20Similarly%2C%20projection%20pursuit%20regression%20models%20have%20%24f(X)%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20g_m(%5Calpha_m%20X)%24%20for%20adaptively%20chosen%20directions%20%24%5Calpha_m%24%2C%20and%20the%20functions%20%24g_m%24%20can%20each%20have%20an%20associated%20roughness%20penalty."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="105" data-gn-md="Penalty%20function%2C%20or%20regularization%20methods%2C%20express%20our%20prior%20belief%20that%20the%20type%20of%20functions%20we%20seek%20exhibit%20a%20certain%20type%20of%20smooth%20behavior%2C%20and%20indeed%20can%20usually%20be%20cast%20in%20a%20Bayesian%20framework.%20The%20penalty%20%24J%24%20corresponds%20to%20a%20log-prior%2C%20and%20%24PRSS(f%3B%20%5Clambda)%24%20the%20log-posterior%20distribution%2C%20and%20minimizing%20%24PRSS(f%3B%20%5Clambda)%24%20amounts%20to%20finding%20the%20posterior%20mode.%20We%20discuss%20roughness-penalty%20approaches%20in%20Chapter%205%20and%20the%20Bayesian%20paradigm%20in%20Chapter%208."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="106" data-gn-md="%23%23%202.8.2%20Kernel%20Methods%20and%20Local%20Regression"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="107" data-gn-md="These%20methods%20can%20be%20thought%20of%20as%20explicitly%20providing%20estimates%20of%20the%20regression%20function%20or%20conditional%20expectation%20by%20specifying%20the%20nature%20of%20the%20local%20neighborhood%2C%20and%20of%20the%20class%20of%20regular%20functions%20fitted%20locally.%20The%20local%20neighborhood%20is%20specified%20by%20a%20kernel%20function%20%24K_%5Clambda(x_0%2C%20x)%24%20which%20assigns%20weights%20to%20points%20%24x%24%20in%20a%20region%20around%20%24x_0%24.%20For%20example%2C%20the%20Gaussian%20kernel%20has%20a%20weight%20function%20based%20on%20the%20Gaussian%20density%20function%0A%24%24%0AK_%5Clambda(x_0%2C%20x)%20%3D%20%5Cfrac%7B1%7D%7B%5Clambda%7D%20%5Cexp%5Cleft%5B-%5Cfrac%7B%7C%7Cx%20-%20x_0%7C%7C%5E2%7D%7B2%5Clambda%7D%5Cright%5D%20%5Ctag%7B2.40%7D%0A%24%24%0Aand%20assigns%20weights%20to%20points%20that%20die%20exponentially%20with%20their%20squared%20Euclidean%20distance%20from%20%24x_0%24.%20The%20parameter%20%24%5Clambda%24%20corresponds%20to%20the%20variance%20of%20the%20Gaussian%20density%20and%20controls%20the%20width%20of%20the%20neighborhood.%20The%20simplest%20form%20of%20kernel%20estimate%20is%20the%20Nadaraya%E2%80%93Watson%20weighted%20average%0A%24%24%0A%5Chat%7Bf%7D(x_0)%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5EN%20K_%5Clambda(x_0%2C%20x_i)%20y_i%7D%7B%5Csum_%7Bi%3D1%7D%5EN%20K_%5Clambda(x_0%2C%20x_i)%7D%20%5Ctag%7B2.41%7D%0A%24%24%0AIn%20general%2C%20we%20can%20define%20a%20local%20regression%20estimate%20of%20%24f(x_0)%24%20as%20%24%5Chat%7Bf%7D_%5Ctheta(x_0)%24%2C%20where%20%24%5Chat%7B%5Ctheta%7D%24%20minimizes"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="108" data-gn-md="%24%24%0ARSS(%5Chat%7Bf%7D_%5Ctheta%2C%20x_0)%20%3D%20%5Csum_%7Bi%3D1%7D%5EN%20K_%5Clambda(x_0%2C%20x_i)(y_i%20-%20%5Chat%7Bf%7D_%5Ctheta(x_i))%5E2%2C%20%5Ctag%7B2.42%7D%0A%24%24"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="109" data-gn-md="and%20%24%5Chat%7Bf%7D_%5Ctheta%24%20is%20some%20parameterized%20function%2C%20such%20as%20a%20low-order%20polynomial.%20Some%20examples%20are%3A%0A-%20%24%5Chat%7Bf%7D_%5Ctheta(x)%20%3D%20%5Ctheta_0%24%2C%20the%20constant%20function%3B%20this%20results%20in%20the%20Nadaraya%E2%80%93Watson%20estimate%20in%20(2.41)%20above.%0A-%20%24%5Chat%7Bf%7D_%5Ctheta(x)%20%3D%20%5Ctheta_0%20%2B%20%5Ctheta_1%20x%24%20gives%20the%20popular%20local%20linear%20regression%20model."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="110" data-gn-md="Nearest-neighbor%20methods%20can%20be%20thought%20of%20as%20kernel%20methods%20having%20a%20more%20data-dependent%20metric.%20Indeed%2C%20the%20metric%20for%20%24k%24-nearest%20neighbors%20is%0A%24%24%0AK_k(x%2C%20x_0)%20%3D%20I(%7C%7Cx%20-%20x_0%7C%7C%20%5Cleq%20%7C%7Cx%5E%7B(k)%7D%20-%20x_0%7C%7C)%2C%0A%24%24%0Awhere%20%24x%5E%7B(k)%7D%24%20is%20the%20training%20observation%20ranked%20%24k%24th%20in%20distance%20from%20%24x_0%24%2C%20and%20%24I(S)%24%20is%20the%20indicator%20of%20the%20set%20%24S%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="111" data-gn-md="These%20methods%2C%20of%20course%2C%20need%20to%20be%20modified%20in%20high%20dimensions%20to%20avoid%20the%20curse%20of%20dimensionality.%20Various%20adaptations%20are%20discussed%20in%20Chapter%206."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="112" data-gn-md="%23%23%202.8.3%20Basis%20Functions%20and%20Dictionary%20Methods"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="113" data-gn-md="This%20class%20of%20methods%20includes%20the%20familiar%20linear%20and%20polynomial%20expansions%2C%20but%20more%20importantly%20a%20wide%20variety%20of%20more%20flexible%20models.%20The%20model%20for%20%24f%24%20is%20a%20linear%20expansion%20of%20basis%20functions%0A%24%24%0A%5Chat%7Bf%7D(x)%20%3D%20%5Csum_%7Bm%3D1%7D%5EM%20%5Ctheta_m%20h_m(x)%2C%20%5Ctag%7B2.43%7D%0A%24%24%0Awhere%20each%20of%20the%20%24h_m%24%20is%20a%20function%20of%20the%20input%20%24x%24%2C%20and%20the%20term%20linear%20here%20refers%20to%20the%20action%20of%20the%20parameters%20%24%5Ctheta%24.%20This%20class%20covers%20a%20wide%20variety%20of%20methods.%20In%20some%20cases%2C%20the%20sequence%20of%20basis%20functions%20is%20prescribed%2C%20such%20as%20a%20basis%20for%20polynomials%20in%20%24x%24%20of%20total%20degree%20%24M%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="114" data-gn-md="For%20one-dimensional%20%24x%24%2C%20polynomial%20splines%20of%20degree%20%24K%24%20can%20be%20represented%20by%20an%20appropriate%20sequence%20of%20%24M%24%20spline%20basis%20functions%2C%20determined%20in%20turn%20by%20%24M%20-%20K%24%20knots.%20These%20produce%20functions%20that%20are%20piecewise%20polynomials%20of%20degree%20%24K%24%20between%20the%20knots%2C%20and%20joined%20up%20with%20continuity%20of%20degree%20%24K%20-%201%24%20at%20the%20knots.%20As%20an%20example%2C%20consider%20linear%20splines%2C%20or%20piecewise%20linear%20functions.%20One%20intuitively%20satisfying%20basis%20consists%20of%20the%20functions%20%24b_1(x)%20%3D%201%2C%20%5Cquad%20b_2(x)%20%3D%20x%2C%20%5Cquad%20b_%7Bm%2B2%7D(x)%20%3D%20(x%20-%20t_m)_%2B%2C%20%5Cquad%20m%20%3D%201%2C%20%5Cldots%2C%20M%20-%202%2C%24%20%0Awhere%20%24t_m%24%20is%20the%20%24m%24th%20knot%2C%20and%20%24z_%2B%24%20denotes%20the%20positive%20part.%20Tensor%20products%20of%20spline%20bases%20can%20be%20used%20for%20inputs%20with%20dimensions%20larger%20than%20one%20(see%20Section%205.2%2C%20and%20the%20CART%20and%20MARS%20models%20in%20Chapter%209).%20The%20parameter%20%24%5Ctheta%24%20can%20be%20the%20total%20degree%20of%20the%20polynomial%20or%20the%20number%20of%20knots%20in%20the%20case%20of%20splines."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="115" data-gn-md="Radial%20basis%20functions%20are%20symmetric%20%24p%24-dimensional%20kernels%20located%20at%20particular%20centroids%2C%0A%24%24%0Af_%5Ctheta(x)%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20K_%5Clambda(%5Cmu_m%2C%20x)%20%5Ctheta_m%3B%20%5Cquad%20(2.44)%0A%24%24%0Afor%20example%2C%20the%20Gaussian%20kernel%20%24K_%5Clambda(%5Cmu%2C%20x)%20%3D%20e%5E%7B-%5Cfrac%7B%7C%7Cx%20-%20%5Cmu%7C%7C%5E2%7D%7B2%5Clambda%7D%7D%24%20is%20popular."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="116" data-gn-md="Radial%20basis%20functions%20have%20centroids%20%24%5Cmu_m%24%20and%20scales%20%24%5Clambda_m%24%20that%20have%20to%20be%20determined.%20The%20spline%20basis%20functions%20have%20knots.%20In%20general%2C%20we%20would%20like%20the%20data%20to%20dictate%20them%20as%20well.%20Including%20these%20as%20parameters%20changes%20the%20regression%20problem%20from%20a%20straightforward%20linear%20problem%20to%20a%20combinatorially%20hard%20nonlinear%20problem.%20In%20practice%2C%20shortcuts%20such%20as%20greedy%20algorithms%20or%20two-stage%20processes%20are%20used.%20Section%206.7%20describes%20some%20such%20approaches."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="117" data-gn-md="A%20single-layer%20feed-forward%20neural%20network%20model%20with%20linear%20output%20weights%20can%20be%20thought%20of%20as%20an%20adaptive%20basis%20function%20method.%20The%20model%20has%20the%20form%0A%24%24%0Af_%5Ctheta(x)%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20%5Cbeta_m%20%5Csigma(%5Calpha_m%5ET%20x%20%2B%20b_m)%2C%20%5Cquad%20(2.45)%0A%24%24%0Awhere%20%24%5Csigma(x)%20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20e%5E%7B-x%7D%7D%24%20is%20known%20as%20the%20activation%20function.%20Here%2C%20as%20in%20the%20projection%20pursuit%20model%2C%20the%20directions%20%24%5Calpha_m%24%20and%20the%20bias%20terms%20%24b_m%24%20have%20to%20be%20determined%2C%20and%20their%20estimation%20is%20the%20meat%20of%20the%20computation.%20Details%20are%20given%20in%20Chapter%2011."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="118" data-gn-md="These%20adaptively%20chosen%20basis%20function%20methods%20are%20also%20known%20as%20dictionary%20methods%2C%20where%20one%20has%20available%20a%20possibly%20infinite%20set%20or%20dictionary%20%24D%24%20of%20candidate%20basis%20functions%20from%20which%20to%20choose%2C%20and%20models%20are%20built%20up%20by%20employing%20some%20kind%20of%20search%20mechanism.%0A%23%202.9%20Model%20Selection%20and%20the%20Bias%E2%80%93Variance%20Tradeoff"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="119" data-gn-md="All%20the%20models%20described%20above%20and%20many%20others%20discussed%20in%20later%20chapters%20have%20a%20smoothing%20or%20complexity%20parameter%20that%20has%20to%20be%20determined%3A%0A-%20the%20multiplier%20of%20the%20penalty%20term%3B%0A-%20the%20width%20of%20the%20kernel%3B%0A-%20or%20the%20number%20of%20basis%20functions.%0AIn%20the%20case%20of%20the%20smoothing%20spline%2C%20the%20parameter%20%24%5Clambda%24%20indexes%20models%20ranging%20from%20a%20straight%20line%20fit%20to%20the%20interpolating%20model.%20Similarly%2C%20a%20local%20degree-%24m%24%20polynomial%20model%20ranges%20between%20a%20degree-%24m%24%20global%20polynomial%20when%20the%20window%20size%20is%20infinitely%20large%2C%20to%20an%20interpolating%20fit%20when%20the%20window%20size%20shrinks%20to%20zero.%20This%20means%20that%20we%20cannot%20use%20residual%20sum-of-squares%20on%20the%20training%20data%20to%20determine%20these%20parameters%20as%20well%2C%20since%20we%20would%20always%20pick%20those%20that%20gave%20interpolating%20fits%20and%20hence%20zero%20residuals.%20Such%20a%20model%20is%20unlikely%20to%20predict%20future%20data%20well%20at%20all."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="120" data-gn-md="The%20%24k%24-nearest-neighbor%20regression%20fit%20%24%5Chat%7Bf%7D_k(x_0)%24%20usefully%20illustrates%20the%20competing%20forces%20that%20affect%20the%20predictive%20ability%20of%20such%20approximations.%20Suppose%20the%20data%20arise%20from%20a%20model%20%24Y%20%3D%20f(X)%20%2B%20%5Cepsilon%24%2C%20with%20%24E(%5Cepsilon)%20%3D%200%24%20and%20%24Var(%5Cepsilon)%20%3D%20%5Csigma%5E2%24.%20For%20simplicity%20here%20we%20assume%20that%20the%20values%20of%20%24x_i%24%20in%20the%20sample%20are%20fixed%20in%20advance%20(nonrandom).%20The%20expected%20prediction%20error%20at%20%24x_0%24%2C%20also%20known%20as%20test%20or%20generalization%20error%2C%20can%20be%20decomposed%3A%0A%24%24%0AEPE_k(x_0)%20%3D%20E%5B(Y%20-%20%5Chat%7Bf%7D_k(x_0))%20%7C%20X%20%3D%20x_0%5D%5E2%20%3D%20%5Csigma%5E2%20%2B%20%5BBias(%5Chat%7Bf%7D_k(x_0))%20%2B%20Var_T(%5Chat%7Bf%7D_k(x_0))%5D%5E2%20%5Ctag%7B2.46%7D%0A%24%24%0A%24%24%0A%3D%20%5Csigma%5E2%20%2B%20%5Cleft%5Bf(x_0)%20-%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_%7B%5Cell%3D1%7D%5Ek%20f(x_%7B(%5Cell)%7D)%5Cright%5D%5E2%20%5Ctag%7B2.47%7D%0A%24%24%0AThe%20subscripts%20in%20parentheses%20%24(%5Cell)%24%20indicate%20the%20sequence%20of%20nearest%20neighbors%20to%20%24x_0%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="121" data-gn-md="There%20are%20three%20terms%20in%20this%20expression.%20The%20first%20term%20%24%5Csigma%5E2%24%20is%20the%20irreducible%20error%E2%80%94the%20variance%20of%20the%20new%20test%20target%E2%80%94and%20is%20beyond%20our%20control%2C%20even%20if%20we%20know%20the%20true%20%24f(x_0)%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="122" data-gn-md="The%20second%20and%20third%20terms%20are%20under%20our%20control%20and%20make%20up%20the%20mean%20squared%20error%20of%20%24%5Chat%7Bf%7D_k(x_0)%24%20in%20estimating%20%24f(x_0)%24%2C%20which%20is%20broken%20down%20into%20a%20bias%20component%20and%20a%20variance%20component.%20The%20bias%20term%20is%20the%20squared%20difference%20between%20the%20true%20mean%20%24f(x_0)%24%20and%20the%20expected%20value%20of%20the%20estimate%E2%80%94%24%5BE_T(%5Chat%7Bf%7D_k(x_0))%20-%20f(x_0)%5D%24%E2%80%94where%20the%20expectation%20averages%20the%20randomness%20in%20the%20training%20data.%20This%20term%20will%20most%20likely%20increase%20with%20%24k%24%2C%20if%20the%20true%20function%20is%20reasonably%20smooth.%20For%20small%20%24k%24%2C%20the%20few%20closest%20neighbors%20will%20have%20values%20%24f(x_%7B(%5Cell)%7D)%24%20close%20to%20%24f(x_0)%24%2C%20so%20their%20average%20should%20be%20close%20to%20%24f(x_0)%24.%20As%20%20grows%2C%20the%20neighbors%20are%20further%20away%2C%20and%20then%20anything%20can%20happen."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="123" data-gn-md="The%20variance%20term%20is%20simply%20the%20variance%20of%20an%20average%20here%2C%20and%20decreases%20as%20the%20inverse%20of%20%24k%24.%20So%20as%20%24k%24%20varies%2C%20there%20is%20a%20bias%E2%80%93variance%20tradeoff."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="124" data-gn-md="More%20generally%2C%20as%20the%20model%20complexity%20of%20our%20procedure%20is%20increased%2C%20the%20variance%20tends%20to%20increase%20and%20the%20squared%20bias%20tends%20to%20decrease.%20The%20opposite%20behavior%20occurs%20as%20the%20model%20complexity%20is%20decreased.%20For%20%24k%24-nearest%20neighbors%2C%20the%20model%20complexity%20is%20controlled%20by%20%24k%24."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="125" data-gn-md="Typically%20we%20would%20like%20to%20choose%20our%20model%20complexity%20to%20trade%20bias%20off%20with%20variance%20in%20such%20a%20way%20as%20to%20minimize%20the%20test%20error.%20An%20obvious%20estimate%20of%20test%20error%20is%20the%20training%20error%20%24%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%7D(y_i%20-%20%5Chat%7By%7D_i)%5E2%24.%20Unfortunately%2C%20training%20error%20is%20not%20a%20good%20estimate%20of%20test%20error%2C%20as%20it%20does%20not%20properly%20account%20for%20model%20complexity."></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="126" data-gn-md="!%5B%5BPasted%20image%2020250825113334.png%5D%5D%0A*Figure%202.11.%20Test%20and%20training%20error%20as%20a%20function%20of%20model%20complexity.*"></div>

<br class="gn-sentinel"><div class="gn-paragraph" data-para-id="127" data-gn-md="Figure%202.11%20shows%20the%20typical%20behavior%20of%20the%20test%20and%20training%20error%2C%20as%20model%20complexity%20is%20varied.%20The%20training%20error%20tends%20to%20decrease%20whenever%20we%20increase%20the%20model%20complexity%2C%20that%20is%2C%20whenever%20we%20fit%20the%20data%20harder.%20However%2C%20with%20too%20much%20fitting%2C%20the%20model%20adapts%20itself%20too%20closely%20to%20the%20training%20data%20and%20will%20not%20generalize%20well%20(i.e.%2C%20have%20large%20test%20error).%20In%20that%20case%2C%20the%20predictions%20%24%5Chat%7Bf%7D(x_0)%24%20will%20have%20large%20variance%2C%20as%20reflected%20in%20the%20last%20term%20of%20expression%20(2.46).%20In%20contrast%2C%20if%20the%20model%20is%20not%20complex%20enough%2C%20it%20will%20underfit%20and%20may%20have%20large%20bias%2C%20again%20resulting%20in%20poor%20generalization.%20In%20Chapter%207%2C%20we%20discuss%20methods%20for%20estimating%20the%20test%20error%20of%20a%20prediction%20method%2C%20and%20hence%20estimating%20the%20optimal%20amount%20of%20model%20complexity%20for%20a%20given%20prediction%20method%20and%20training%20set."></div>